Real-Time Rendering

第1章 介绍
实时渲染是跟用计算机快速制造图片相关的。这是计算机图形学交互性最高的领域。一张图片出现到屏幕上，然后观察者做出行动或反应，而这个反馈会影响到下一张生成的图片。这个反馈和渲染的循环以足够快的频率进行，观测者并不会看到单张图片，而会沉浸到一个动态的过程中。
图片显示的频率由每秒的帧数（frames per second, FPS）或者赫兹(Hertz, Hz)来测量。每秒1帧，交互性很低：用户等待每张新的图片到来是很痛苦的。在6FPS左右，交互性的感觉开始增长。游戏的目标是30，60，72或者更高的FPS；在这些速率下，用户会专注于动作和反应。
电影投影仪显示帧率为24FPS但用一个快门系统展示每帧2-4次来避免闪烁。这个刷新率是独立于显示频率的，且用赫兹表示。快门让每帧3倍亮度就需要72Hz的刷新率。LCD监控器也是将显示频率与刷新率区分开的。

一些基本的数学概念和术语
此处我只记录一下我之前不太熟悉的术语
shading, shader等派生出的词，根据上下文不同可能有不同的意思。可能表示计算机生成的视觉外观（比如shading model, shading equation, toon shading），也可能表示渲染系统中的一个可编程的组件（比如"vertex shader", "shading language"）。具体的含义需要根据上下文而定。

在网站realtimerendering.com上，有许多补充材料，如果书中有看不懂或者想更加深入了解的地方，可以去这个网站上继续寻找。

第2章
图像渲染管线（流水线）(The Graphics Rendering Pipeline)
这章节将试试图像的核心组件展示出来，成为图像渲染管线，也可以简称为“管线(pipeline)”。这个管道的主要功能是生成或渲染，一个二维图像，给定一个虚拟镜头，三维物体，光源以及其他。
frustum：圆锥截头体，复数形式：frusta

管线流程：
1.结构（Architecture）
管线阶段是并行执行的，其中的每个阶段依赖于前一个阶段的结果。理想情况下，一个没有管线化的系统，被分为n个管线化的阶段后，可以产生系数为n的加速比。这种性能上的提升是管线化的原因。
（个人提问：为什么划分为n个阶段后，管线的加速比可以理想情况下达到n）
举个例子，多个三明治可以用分工明确的一群人快速准备好——一些准备面包，一些添加肉，一些添加配菜。每个人将结果顺序地传递给下一个人，然后马上开始对下一个三明治的处理。
（个人解答：所以这个加速比实在多张图处理时候的加速，对于一张图，实际上没有加速。这就跟指令流水线的加速类似，一条指令可能由5个阶段组成，然后当多条指令执行的时候，可以实现并行，即多条指令的不同部分在同时执行）
类似指令的流水线执行，管线阶段虽然并行执行，但是每个阶段的时间需要不小于最慢阶段的时间。而这个最慢的阶段，也被称为瓶颈。瓶颈决定了整个流程的速度。比瓶颈快的瓶颈后的阶段会出现没有等待结果的“饥饿”情况。
实时渲染可以大致分为4个主要阶段：Application, geometry processing, rasterization以及pixel processing
这其中的有的阶段可能再细分为一个流水线，再分为多个子阶段，有的阶段可能可以部分并行。
（个人提问：Application阶段是干什么的？）
（个人解答：这个应该在后面的内容有提到，不过，我先查了下内容，Application阶段将镜头位置，光照，下一阶段需要的模型原型导入）
在具体执行的时候，可能将时间消耗较短的阶段组合成一个新的单元，也可能将一个耗时较长的阶段分拆为更多的子阶段。
渲染速度可以用FPS来表示。
FPS可以用来表示某个特定帧的速率，也可以用来表示一段时间的平均性能。Hertz(赫兹)用于硬件，比如显示器，设置为固定频率。
application阶段由application驱动，通常在通用CPU上运行的软件中应用。这些CPU通常是多核的可以处理并行多线程。这让CPU可以高效运行application阶段的多个任务。
一些一般在CPU上运行的任务包括：碰撞检测，全局加速算法，动画，物理模拟以及许多其他取决于application类型的任务。
几何处理，处理的是变换，投影以及其他一些类型的几何问题。这个阶段计算画的是什么，怎么画，在哪里画。几何处理阶段通常在GPU上运行，这包含了许多可编程核以及一些固定运行的硬件。
光栅化阶段将三角形的三个点作为输入，并找到所有在三角形中的像素，然后将这些像素送到下一个阶段。
像素处理阶段对每个像素执行一个程序，算出每个像素的颜色，也许会用到深度测试检测是否可见。也可能需要用到颜色融合，将新计算出的颜色与之前的颜色融合。
光栅化和像素处理阶段全部都在GPU上运行。所有这些阶段以及其内部的流水线会在之后的4个部分中讨论。关于GPU如何处理这些阶段，在第三章会有更多细节。

2.2 The Application Stage
开发者可以完全掌控application stage中发生的情况，因为通常这个阶段在CPU上执行。因此开发者完全可以决定其执行，并在之后修改以提升性能。这里的改动也会影响到之后阶段的性能。比如，application阶段的算法或设定可以减少之后渲染的三角形数量。
不过，有一些application阶段的工作可以用GPU来完成，这会用到一个独立的模式叫做compute shader（计算着色器）。这个模式将GPU当做高并行程度的通用处理器，而忽略其为了渲染图像而特化的功能。
application阶段结束时会将要渲染的几何图形送到几何处理阶段。这些就是渲染原型，比如点、线、三角形，最后也许会显示在屏幕上。这也是application阶段最重要的任务。
这个阶段基于软件执行的结果就是它不能被分为子阶段，而其他三个阶段可以。不过，为了提升性能，这个阶段通常会在多个处理器核上并行执行。在CPU设计中，这叫做超标量构造，因为在同一阶段同一时刻，它可以执行多个处理（超标量的概念在计算机组成原理中有印象提到过）。18.5章节战士了多种用多处理器核的方法。

2.3 几何处理
GPU上的几何处理阶段对大部分每个三角形或每个点的运算负责。
这个阶段又可以继续分为如下几个功能阶段：
点着色，投影，剪裁，屏幕映射

2.3.1 点着色
两个主要任务：计算点的位置；计算可能需要的点的输出数据，比如法向量或纹理坐标。
传统意义上，点着色是通过在每个点的位置和法向量上打上光，然后计算每个点的最终颜色，并将这些颜色在三角形中进行插值。
因此，点处理单元也叫做点着色器。
现在的点着色器是一种更一般的单元，用于将与每个点关联的数据设置好。比如，在4.4和4.5章节中点着色器用里面的方法让物体动起来。
开始是点的位置计算。到屏幕上，模型被转换为不同的空间或坐标系统。初始，模型在它们自己的模型空间中。一个模型可以跟一个模型转换关联，进而可以进行将其放置。且一个模型可以有多个模型转换，因此同一个模型可以在同一个场景中有多个位置、朝向、大小不同的副本（也叫做实例）。
模型转换实质上是点和法向量的转换。模型坐标（基于物体的坐标系）经过模型转换，变为世界坐标（世界空间体系内）。转换后，所有的模型都会在同一个世界空间中存在。
世界中有镜头的位置和方向。镜头和所有模型都用view transform转换，为了实现投影和剪裁。
view transform的目的在于将镜头放到原点，看向-z方向，y为竖直相上，x向右。有一些书中喜欢用+z方向。两者实际上是相同的，两者的转换也是很容易的。经过view transformation后，空间被划分为镜头空间，或叫做view space（视觉空间），或眼睛空间。
（个人理解：所以model transformation就是把几何模型放到了世界空间中，镜头也在这个世界空间中；而view transformation就是将这个世界空间中的所有物体，包括镜头，转换为以镜头为原点的一个正交坐标系下的坐标，即转换到镜头空间中的坐标）
model tranform和view transform都用4 * 4矩阵完成，这也是第4章中的内容。
要明白一点，点的坐标和法向量可以根据开发者的喜好而按任意方式进行转换。
点着色的第二种输出是物体的外观，包括其材质，光打到物体上的效果。材质和光可以用许多方式模型化，从简单的颜色到精细的物理描述。
（个人理解，这个就是材质相关的一些参数，比如GAMES202中BRDF函数，这个部分应该是用于后续的光线追踪之类的算法，模拟真实物理外观，而不是一个只有位置和外形的世界，那么在GAMES101和202中的那些没有着色的图，估计就是不考虑材质子类的参数，只考虑位置和形状的渲染结果吧）
决定光打到材料上的效果的操作就是着色。它包括计算物体上的点的着色方程。一般情况下，这些计算中的一部分会在模型顶点的几何处理阶段进行，另外一些可能会在per-pixel processing阶段进行。
顶点着色的结果（可以是颜色，向量，纹理坐标，以及其他着色数据）会送到光栅化和像素处理阶段进行插值，用于计算表面的着色。
在第3章和第5章会深入讨论GPU着色器的顶点着色。
作为顶点着色的一部分，渲染系统会进行投影然后裁剪，将视觉空间变换为一个单元立方体([-1, 1] * [-1, 1] * [-1, 1])，这个立方体被叫做规范视觉体。先投影，这是在GPU的顶点着色器完成的。
投影又分为正交投影和透视投影。
（个人理解，这里GAMES101做了详细的讲解，以及讲解了如何从正交投影变换到透视投影。还有这两种投影的变换矩阵如何求解）
之所以叫做投影，是因为这波操作只有，z不在图片中存储了，而是在z-buffer中存储。这下，模型就从3维投影到了2维。

2.3.2 可选的点处理
点处理的管线就如之前描述。而这个过程完成之后，GPU中还可以进行许多可选的阶段，顺序如下：镶嵌，几何着色，流输出。这些阶段的应用取决于硬件情况（不是所有的GPU都有这些），以及编程者的喜好。他们彼此是独立的，一般情况下不太常用。第3章中会做一些讨论。
镶嵌的一个例子：你有一个弹力球物体，如果你用三角形模拟，那么近距离看，会很糟糕，尤其是轮廓。如果我们用更多的三角形模拟，得不偿失，会用掉很多算力和内存，但可能只算了一幅图上的一小部分像素。这时，如果我们用镶嵌，就可以用一个合适数量的三角形构造出曲面。
（个人理解，我查了一下之前GAMES101的课程笔记，里面提到了tessellation，翻译为曲面细分，就是根据情况动态调整三角形的拆分情况，远了就不拆，近了再根据需要拆分出更多的三角形）
tessellation阶段自己就包含多个阶段：hull shader, tessellator和domain shader——将点的集合变为更大的点的集合，并由此产生新的三角形集合。
下一个可选的阶段是geometry shader（几何着色）。这个着色器比tessellation着色器出现得要早，所以在GPU中更常见。几何着色器将多种原型输入，产生新的点。但是它比曲面细分着色器要简单，因为它输出的原型很有限。几何着色器最火的应用是产生粒子。想象一下模拟烟花爆炸。每个火球都可以用一个点来表示，几何着色器可以将每个点转换为一个方形（由两个三角形构成）朝向观测者并覆盖多个像素，这给我们提供了一个更可行的着色原型。
最后一个是stream output。则是让我们将GPU用作一个几何引擎。这个阶段我们可以将这些数据不向后面阶段输出，而是直接输出为一个数组用于后续的处理。这些数据可以之后让CPU来用或者是GPU。这个阶段通常用于模拟粒子，比如烟花。
这三个阶段的顺序是tessellation, geometry shading, stream ouput，且每个阶段都是可选的，取决于我们的用法，如果我们继续向之后的管线下行，那么我们就需要检测这些齐次坐标对镜头是否可见。

2.3.3 裁剪
把在单元立方体中的原型保留，剩下的部分裁剪掉
（虽然书里叙述了不少，关于透视投影空间的，但是最后表达的意思大概就是裁掉不在单元立方体中的部分）

2.3.4 屏幕映射(screen mapping)
只有裁剪后的部分进入屏幕映射阶段。
x, y坐标映射到屏幕上的坐标，与z值一起被称为窗口坐标（window coordinates）。
（OpenGL里z值为[-1, 1]，DirectX中z值为[0, 1]）
窗口坐标被传入到光栅化阶段
坐标转换到像素，最左边的像素是0.0，像素的中间是0.5，所以像素[0, 9]实际上覆盖了范围[0.0, 10.0]
转换方式为 d = floor(c), c = d + 0.5，其中d是像素下标，c是像素中的连续值。
OpenGL是把左下角当做最低值点，DirectX有时把左上角当做最低值点。
这个在迁移API时需要注意。

2.4 光栅化
(个人理解：应该叫栅格化)
这个阶段的任务是找到所有在原型中的像素（图片元素的缩写，picture elements）。
这个阶段又被分为两个子阶段：三角形设置，三角形遍历。
这个阶段也叫做扫描变换，将二维的点变换到屏幕空间上，需要考虑到每个点的z值以及其他很多信息。

2.4.1 三角形建立
微分，边的方程，其他数据在这个阶段被计算也许会用于之后的三角形遍历和几何阶段产生的着色数的插值。

2.4.2 三角形遍历
检测三角形中的像素，产生三角形覆盖的像素碎片。在5.4章节会有更复杂的取样方式。

2.5 像素处理
这个阶段分为像素着色和合并两个部分。

2.5.1 像素着色
每个像素的着色运算在这个阶段进行，将插值的着色数据作为输入。结果为一个或多个颜色，传入下一个阶段。
像素着色阶段在可编程GPU核上运行。（三角形建立和三角形遍历，通常在精细的，硬连线的硅片上进行）
程序员需要提供一个像素着色器（或碎片着色器，在OpenGL这样称呼），其中包含了需要进行的运算。
在这个阶段，大量的技术可以应用，其中最重要的一种是纹理。纹理会在第6章详细讲到。可以简单理解为，将图片贴到物体表面。
这个阶段的结果是每个碎片的颜色值，传入下一个子阶段。

2.5.2 合并
每个像素中的信息存在颜色缓存(color buffer)中，是一个矩形的颜色向量(每种颜色的红、绿和蓝色组成情况)。合并阶段的任务是将像素着色阶段的碎片颜色与现在在缓存中的颜色组合。
这个阶段也叫做ROP，raster operations(pipeline)或render output unit。运行这个阶段的GPU子单元通常不是完全可编程的。但是是高度可设置的，可以实现很多效果。
这个阶段也会处理可见性的问题。这意味着，当整个场景渲染的时候，颜色缓存中存储了场景中镜头视角下可见的物体原型的颜色。这个通常用z-buffer算法完成。z-buffer的规模和形状与颜色缓存相同，对每个像素存储了其距离最近的物体原型的z值。由于z-buffer中只保留了z值，部分透明的原型不能用这个算法。有透明的原型必须在不透明的原型之后渲染，且以从后向前的顺序进行，或者用一个顺序无关的算法（第5.5章节会讲）。无法处理透明物体是基础z-buffer的一大不足。
除了颜色缓存和z-buffer，还有其他的一些通道和缓存，可以用于过滤和捕捉碎片信息。alpha通道存储了每个像素的透明度。
stencil buffer（模板缓存）用于记录每个渲染的原型的位置，通常每个像素用8位。可以用于产生一些特效。
这些流水线最后的功能叫做光栅操作（ROP），或融合操作。
framebuffer通常包含了系统中的所有缓存。

2.6 流水线总览
举了个例子将整个流水线过了一遍。

总结
这个流水线是API和图形学硬件为了实时渲染进化了几十年的结果。不过要知道，这并不是唯一可行的流水线：离线的流水线会用不同的路径。而渲染电影常常用的是微多边形流水线（micropolygon pipelines），但光线追踪和路径追踪最近接管了这部分。这些技术，在11.2.2章节中会提到，也可以用于建筑或设计的虚拟预览。
许多年来，开发者能应用的唯一方法就是这里描述的，一个由使用中的图形学API定义的固定功能的流水线。之所以说固定功能，是因为应用这些的图形学硬件有一部分不能灵活编程。主流固定功能的机器的最后一个例子是Nintendo的Wii，2006年问世。可编程GPU另一方面让流水线中的多个子阶段中进行的操作变得可以重新定义。对于第4版书，我们假设所有的开发都在可编程GPU上进行。

第3章 图像处理单元（The Graphics Processing Unit）
从历史上看，图像加速从对三角形中的像素的插值并显示这些值开始。包括能获取图像数据的能力，让纹理应用于表面。为插值和z-depths检测添加硬件提供了内置的可见性检测。由于它们被频繁使用，这个过程被专门做成了硬件以提升性能。渲染管线的更多部分，以及每条管线的更多功能在之后的几代被加入。专用的图像硬件比CPU的唯一优势是速度，而速度至关重要。

接着这部分大概讲了一下GPU发展的历史，从不可编程到大部分可编程，灵活性和可编程性成为主流。

获取数据的延迟是一个影响性能的重要因素。

3.1 平行数据架构
为了减少延迟，CPU采取了很多策略，比如分支预测，指令重排，寄存器重命名，和预取高速缓存。
但是GPU采取了不同的方式。GPU芯片中的一大部分区域用于大量的被称为着色器核的处理器，通常上千个。
GPU是一个流处理器，有序的多组相似数据会轮流处理。由于相似性，GPU可以大量并行地处理数据。
GPU优化的是吞吐量，定义了数据处理的最大速率。
但快速处理的代价是，芯片中用于缓存和控制逻辑的区域会变小，每个着色核的延迟通常要比CPU处理器的大得多。
GPU的运行策略是，大量的核运算同一种运算，完成后再转向别的。这样的策略能让整体执行时间大幅降低。
GPU采用的设计是将执行的执行逻辑和数据分离。这叫做单指令，多数据（Single instruction, multiple data，SIMD），这样可以在固定数量的着色器程序上执行锁定步骤的同一条指令。SIMD的好处是，与用独立逻辑和派遣单元去执行每个程序相比，大幅降低了需要专用于处理数据和切换的硅片。
以咱们的2000个碎片(fragment)为例，在现代GPU下，每个碎片的每个像素着色调用叫做一个线程。这个线程和CPU的线程不同，它包含了着色器输入值的一点内存和以及着色器执行需要的任意寄存器空间。
用同样着色器的程序的线程捆绑为组，在NVIDIA中叫做warps，而在AMD中叫做wavefronts。由一些GPU着色核规划warp/wavefront的执行，用SIMD处理，从8到64。每个线程会映射到一个SIMP lane(lane, 车道，轨道)。
我们有2000个线程要执行，NVIDIA GPU的warp包含32个线程，这意味着会有2000/32 = 62.5个warp，即63个warp会被分配，1个wap会在半空状态。一个warp的执行与一个GPU处理器例子的执行类似。所有的32个处理器上，着色程序锁步(lock-step)执行。当遇到内存抓取，所有线程都会遇到同时它，因为所有都同时执行。在遇到内存抓取指令时，应该停下等待结果，但实际上是需要停下的warp被存储，换为一个32线程的不同warp，在32个核上执行。这个换出与单核系统一样快，因为这个过程中实际上没有数据的换出，而是将不同的核指向了不同的线程组。每个线程有自己的寄存器，而每个warp会最终它在执行的指令。
而warp的换出会导致一点点延迟，但是代价是很低的。不过也有很多方法来优化这个执行，但是warp-swapping是所有GPU都在用的隐藏延迟机制。这个过程运行得多高效受到多个因素影响。比如，如果线程比较少，进而能创建的warps也比较少，这就会让隐藏延迟有困难。
着色程序的结构也是影响效率的一个重要因素。其中一个主要的因素是每个线程使用寄存器的数量。在我们的例子中，我们假设2000个线程在GPU上同时都存在。每个线程上着色程序需要的寄存器越多，线程数就越少，warps就越少。而warps的短缺意味着停等不能通过换出来缓解。在运行的warp被称作"in flight"（在线上），这个数量被称为占用量。高占用量意味着高性能表现。内存抓取的频率也会影响到需要多少隐藏延迟。Lauritzen列出了占用量如何受寄存器数量和着色器使用的公共内存影响。
Wronski讨论了理想的占有率取决于着色器进行的指令类型。
另一个影响全局效率的因素是动态分支，由if语句和循环引起的。如果if语句让一部分线程，哪怕是一个线程执行了不同的分支，那么这意味着，这个warp都要执行两个分支，并由特定的线程舍弃不需要的结果。这个问题叫做“线程分歧”，一些线程需要执行循环迭代或进行"if"路径，而warp中的另一些线程不需要，这就让那些线程在这期间空闲了下来。
所有的GPU都用到了这个结构想法，结果导致了系统有样的限制但是大量的每瓦特计算能力。理解这个系统是如何运作的可以帮助你更加高效地是好用这些算力。下面的章节，我们将讨论GPU如何应用这些渲染管线，可编程的shaders如何运转，以及每个GPU阶段的功能和进化。
GPU渲染管线的应用：
Vertex Shader(点着色，完全可编程)->Tesselation(曲面细分，完全可编程)->Geometry Shader(几何着色，完全可编程)->Clipping(裁剪，完全固定)->Screen Mapping(屏幕映射，可设置)->Triangle Setup & Traversal(三角形建立和遍历，可设置)->Pixel Shader(像素着色，完全可编程)->Merger(合并，可设置)

3.2 GPU管线概览
GPU应用概念上的图像处理，光栅化，像素处理管线阶段在第2章中描述。这些根据其可设置程度或可编程程度分为不同的硬件阶段。如前面的描述。在第2章中，这些物理阶段的划分与功能阶段的划分不同。
我们这里描述了GPU的逻辑模型，通过API暴露给开发者的模型。在第18章和第23章中讨论的，这个逻辑管线的应用，物理模型，是由硬件供应商决定的。一个逻辑模型中的固定功能的阶段可能是由GPU中在一个相邻的可编程阶段加入指令后完成。一个简单的管线中的程序可以被分为由独立子单元执行的元素，或者由完全由一个独立的流程执行。逻辑模型可以帮助你搞明白什么会影响性能，但是它不应该跟GPU实际应用管线的过程混淆。
顶点着色器是完全可编程的阶段，用于之后应用到几何处理阶段。几何着色器是一个完全可编程的阶段，基于原型的顶点（点，线或三角形）。可以用于每个原型的着色操作，删除原型或添加原型。曲面细分阶段和几何着色器都是可选的，并不是所有的GPU都支持它们，特别是移动设备。
裁剪，三角形建立，三角形遍历解读那是由固定功能的硬件运行的。屏幕映射由窗口，视口设置，在内部定义了一个简单的规模和重定位。像素着色器阶段是完全可编程的。虽然合并阶段并不可以变成，但是是高度可设置的，而且可以用来运行很多种操作。在运行“合并”功能阶段时，它负责了修改颜色，z-buffer，融合，模板，以及其他任何与输出想滚的缓存。像素着色器的执行和合并阶段，一起构成了概念上的像素处理阶段，在第2章中展示过。
一直以来，GPU管线从硬编码操作向提升灵活性和可控性进化。对可编程着色器阶段的介绍，是这个进化过程中最重要的一步。下一节，我们将描述一些许多可编程阶段的共性。

3.3 可编程着色器阶段
现代的着色器程序用的是一个统一的着色器设计。这意味着点、像素、集合和曲面细分相关的着色器有着一个共同的编程模型。在内部，他们有相同的指令集架构(instruction set architecture, ISA)。应用这个模型的处理器在DirectX中叫做通用着色器核，一个有这种核的GPU可以认为有一个统一的着色器架构。这种架构背后的理念是着色器处理器可以用作多种角色，而GPU可以根据需要分配它们。比如，一组三角形网格会比一个由两个三角形构成的 正方形需要更多的点着色器来处理。一个有多个独立的点、像素着色核池的GPU意味着理想的工作分配可以让所有核工作是严格预设的。有统一的着色器核，GPU可以决定如何平衡负载。
描述整个渲染器编程模型超过了这本书的范畴，而且有许多文档，书和网站已经做了这件事。着色器可以用类C的着色语言进行编程，比如DirectX的High-Level Shading Language(HLSL)和OpenGL Shading Languag(GLSL)。DirectX的HLSL可以编译为虚拟机字节码，也叫做intermediate language(IL或DXIL)，用以提供硬件独立性。中间代表也可以允许着色器程序离线编译和存储。这个中间语言由驱动转换为特定GPU的ISA。控制台编程通常避免了中间语言的步骤，因为在那里只有系统的ISA。
基础的数据类型是32位的单精度浮点数常量和向量，虽然向量只是着色器代码中的衣服分，且不受到之前列出的硬件支持。在现代GPU上，32位的整数和64位浮点数也原本就受到支持。浮点数向量通常包含了诸如坐标(xyzw)，法向量，矩阵行，颜色(rgba)，或纹理坐标(uvwq)的数据。整数最常用于计数器，下标和掩码(bitmasks)。合并的数据类型比如结构体，数组，矩阵也是支持的。
一个绘制调用(draw call)调用了图形学API绘制一组原型，进而让图形学管线开始进行并运行它们的着色器。每个可编程着色器阶段都有两个类型的输入：统一输入(uniform inputs)，在draw call中是常量的数（不过在不同的draw calls中可能会改变），以及变化的输入(varying inputs)，来自三角形顶点的数据或者来自光栅化的数据。比如，一个像素着色器也许会将光源作为统一输入，而三角形表面的位置对每个像素都会改变，因此是可变化的。纹理是一种特殊的输入，曾经它是一个应用于表面的颜色图片，但是现在它可以是任何大数组数据。
底层的虚拟机体用了特殊的寄存器用于不同类型的输入和输出。用于统一输入的常量寄存器的数量远大于变量输入或输出的寄存器数量。之所以这样，是因为每个点或像素的可变化的输入和输出需要分开存储，因此自然地，可以用的数量就会有限制。统一输入会存储一次，然后在draw call中对所有点或像素复用。虚拟机也有一个通用目的临时寄存器（general-purpose temporary registers），用于暂存空间（scratch space）。所有寄存器可以用数组下标，即用临时寄存器中的整数。
在现代GPU上，常用的图形学计算中的操作可以高效地执行。着色语言通过运算符，比如*和+，将最常用的运算暴漏出来。为了GPU的优化，剩余的通过内在函数(intrinsic functions)暴露，比如stan(), sqrt(), log()，以及其他。更复杂运算的函数也存在，比如向量单位化和矢量反射，点积，矩阵转置以及矩阵行列式的值的运算。
术语流控制指的是让指令分支以改变代码执行的流。与流控制相关的指令用于实现高层次语言的构造，比如"if"和"case"语句，还有各种各样的循环。着色器支持两种流控制，静态流控制分支基于统一输入的值。这意味着这段流代码在draw call中是不变的。静态流控制的一个最起码的好处是让同样的着色器可以在很多不同的场景中使用（比如，各种各样的光源）。没有线程分歧，因为所有的调用都会走同一条代码路径。动态流控制基础可变输入的值，这意味着每个碎片可以在执行代码时不同。这比静态流控制要强大多了，但是是以性能为代价的，特别是当你的代码流在着色器调用间不规律地改变时。

3.4可编程着色和API的进化
可编程着色框架的想法可以追溯到1984年库克(Cook)的着色树(shade trees)。The RenderMan Shading Language在20世纪80年代从这个理念发展出来。在电影着色过程中，这个语言还在使用，以及一些其他的进化的特性，比如Open Shading Language(OSL)。
用户级的图像硬件第一次由3dfx Interactive在1996年10月1号成功引进。他们的Voodoo图像卡可以高质量高效渲染游戏Quake，让它们迅速被采用。这个硬件用到了一个整体固定功能管线。在GPU自身支持可编程着色器前，有过多次通过多轮渲染而实时实现可编程着色操作的尝试。1999年，Quake III: Arena脚本语言是第一个在这个领域广泛流传的商业成功。而本章节开头提到的，NVIDIA的GeForcce256是第一个可以被称为GPU的硬件，但是它不是第一个可编程的。虽然，它可以设置一些参数。
在2001年初，NVIDIA的GeForce 3是第一个支持可编程点着色的GPU，通过DirectX 8.0和OpenGL的拓展暴露。这些着色器用一种类似汇编的语言实现可编程，类汇编语言会由驱动在线转换为微码。DirectX 8.0中也包含了像素着色器，但是像素着色器缺乏实际的可编程性————受支持的有限的“程序”在驱动中转换为纹理融合状态，轮流和硬件的“寄存器结合器”连在一起。这些程序不仅长度受到限制（12个指令或者更少）而且缺乏重要的功能。依赖的纹理读入和浮点数对于真正的可编程性是至关重要的，在Peercy et al.对RenderMan的研究中提到。
这个时候的着色器还不允许流控制（分支），因此条件需要通过计算两种情况然后在结果中选择或者插值这样模拟来实现。DirectX定义了一个Shader Model（SM）的概念以区分不同着色器能力的硬件。2002年，DirectX出现，包含了Shader Model 2.0，其特性是对点和像素着色的真实可编程性。相似的功能在OpenGL下也可以用多个拓展来暴露。新增了支持任意的依赖纹理读取和对16位浮点数值的存储的功能，终于满足了Peercy et al.定义的一系列要求。着色器资源中的限制，比如指令、纹理和寄存器，增多了，而着色器可以实现更加复杂的效果。流控制的支持也被添加。着色器增加的复杂度和长度让汇编编程模型变得繁琐。幸运的是，DirectX 9.0也包含了HLSL。这个着色语言是由微软与NVIDIA合作开发的。几乎同时，OpenGL ARB(Architecture Review Board)发布了GLSL，对OpenGL而言，这是相当熟悉的语言。这些语言受到了句法和C编程语言设计哲学的深刻影响，也包含了RenderMan Shading Language中的元素。
2004年，着色器模型3.0发布，加入了动态流控制，这让着色器大幅增强。它也将可选特性变为了要求，更是提升了资源限制并添加了在点着色器中的纹理读入限制。当新一代游戏终端发布（微软的Xbox 360和说你的PLAYSTATION 3系统），它们有着色模型3.0级别的GPU。Nintendo的wii中断是最后一个值得关注的固定功能GPU，在2006年后期发布。至此，纯粹的固定功能管线的时代就结束了。着色语言进化到了一个多种工具可以用来创造和管理它们的地步。
而可编程性的下一步在2006年也几乎走完了。着色模型4.0，包含在DirectX 10.0中，发布了许多主要的特性，比如几何渲染和流输出。着色器4.0中包含了一个对所有着色器（点，像素和几何）都统一的设计，就如之前所讲到的。资源限制又增多了，且对整数类型的支持也被添加了。在OpenGL3.3中的GLSL 3.30用到了一个类似的着色模型。
在2009年，DirectX 11和着色器模型5.0发行了，添加了曲面细分阶段的着色器和计算着色器（compute shader），也叫做DirectCompute。发行版还注意了跟有效地支持CPU多处理，在第18.5节会讨论这个。OpenGL在4.0版本添加了曲面细分，并在4.3版本添加了计算着色器。DirectX和OpenGL进化得不同。两者都设置了对特定发行版本的支持需要一定级别的硬件支持。微软控制DirectX API，直接与独立硬件供应商(independent hardware vendors，IHVs)合作，比如AMD，NVIDIA，决定暴露的特性是什么。OpenGL是由一个财团的软件和硬件供应商发展的，由一个公益组织Khronos Group管理。因为有多个公司参与，API特性通常是在DirectX的介绍后若干时间，在OpenGL的发布中出现。不过，OpenGL允许拓展，供应商定制或更具一般性的，这就可以让GPU最新功能可以在官方发布支持前就能用到。
下一个API的重大改变是在2013年，AMD对其Mantle API的介绍中开始的。与DICE游戏开发商合作，Mantle的想法是剥离许多图像驱动的开销，而让控制权给开发者。与这个重构一同出现的还有对高效CPU多处理的更多支持。这类新的API致力于大量减少CPU在驱动上花费的时间，以及让CPU更高效地支持多处理器。Mantle中的先进理念被微软采纳，并在2015年的DirectX 12中实现。注意，DirectX 12并不致力于将新的GPU功能暴露，它与DirectX 11.3中的硬件特性是相同的。两者API都可以用于将图像发送给虚拟现实系统，比如Oculus Rift和HTC Vive。但是DirectX 12对API进行了彻底的重新设计，能更好映射到现代GPU架构。低开销驱动程序在CPU驱动是瓶颈的应用中或是在更多的用于图像的CPU处理器会提升性能的场景中很有用。从更早的API迁移是困难的，一个简单的应用会导致低性能表现。
在2014年，苹果发行了它自己的低开销API，叫做Metal。Metal是第一个移动设备上可以获得的API，比如IPhone 5S和IPad Air。一年后，通过OS X El Capitan，Macintoshes得到了接口。不只是效率，减少CPU的使用还节约了能量，这是移动设备上的一个重要因素。这个API有它自己的着色语言，可用于图像和GPU计算程序。
AMD将其在Mantle上的工作捐献给了Khronos Group，这个组织在2016年早期发布了它自己的新API，叫做Vulkan。如同OpenGL，Vulkan可以在多个操作系统上运行。Vulkan用的是新的高级中间语言，叫做SPIRV，它可以用于着色器表示和一般的GPU计算。提前编译的着色器是可以迁移的，可以在任何支持其必要能力的GPU上运转。Vulkan也可以用作非图像化的GPU计算，因为它不需要一个显示窗口。Vulkan和其他低开销驱动的一个重大区别是它可以在很多系统运行，从工作站到移动设备。
在移动设备上，现在的规范是用OpenGL ES。"ES"代表了Embedded Systems，这些API是适用于移动设备开发的。标准的OpenGL那是还在某些调用结构上笨重而缓慢，且需要一些很少用到的功能的支持。在2003年发布，OpenGL ES 1.0是OpenGL 1.3的一个精简版，描述一个固定功能管线。接着，DirectX与那些支持它的图像硬件发布，开发移动设备支持的图像并不是那时的主流。比如，第一台iPad，在2010年发布，用的是OpenGL ES 1.1。2007年，OpenGL ES 2.0的特性才放出，提供了可编程的着色。这是基于OpenGL 2.0的，但是没有固定功能的组件，因此是不能与OpenGL ES 1.1后向兼容的。OpenGL ES 3.0在2012年发布，提供了一些诸如多渲染目标，纹理压缩，转换反馈，实例化，以及一大堆纹理格式和模式的功能，也有一些着色语言的提升。OpenGL ES 3.1添加了计算着色器，3.2添加了几何和曲面细分着色器，以及一些其他特性。第23章会更细致地讨论移动设备的架构。
OpenGL ES的一个分支是基于浏览器API的WebGLSL，通过JavaScript调用，在2011年发布，这个API的第一个版本是可以在大部分的移动设备上使用，因为它在功能上等同于OpenGL ES 2.0。跟OpenGL相同，拓展让更多先进的GPU特性可以使用。WebGL 2假设OpenGL ES 3.0支持。
WebGL特别适合在教室中实验或使用一些特性：
它是跨平台的，在所有个人计算机和几乎所有移动设备上支持。
通过浏览器实现驱动支持。即使一个浏览器不支持特定的GPU或拓展，通常会有其他浏览器支持。
代码是解释型的，而不是编译型的，开发只需要一个文本编辑器。
大部分浏览器都有内置的调试器，在任何网站上运行代码都可以检测。
程序可以通过将它们上传到网站上，例如Github上，而被部署。
高层次的场景图像和效果库，比如three.js，让大量相关的效果可以更加简单地写代码实现，比如阴影算法，后期处理效果，基于物理的着色，以及延迟渲染。

3.5 顶点着色器
顶点着色器是功能管线中的第一个阶段。虽然这是第一个可以直接由开发者控制的阶段，它其实只相当于这个阶段前的数据控制。在DirectX中称为输入汇编器中，多个数据流编织到一起，形成多组顶点和原型，输入到管线。比如，一个物体可以由一组位置和一组颜色表示。输入汇编器通过用位置和颜色创造顶点产生物体的三角形（或者线，或者点）。第二个物体也可以用同样的位置（用一个不同的模型转换矩阵）以及一组不同的颜色用作它的表示。数据表示的细节在16.4.5章节会细致讨论。输入汇编器的也支持实例化。这就允许一个物体绘制多次，每个实例中有一些可变的数据，都用一个draw调用完成。实例化的使用在18.4.2章中会讲到。
三角形网格用一系列的点表示，每个在模型的表面都与一个特定的位置关联。除了为孩子，还有一些其他的可选参数，与每个点关联，比如颜色和纹理坐标。表面法向量也在顶点网格中定义，这看起来也许有点怪。数学上，每个三角形都有一个良好定义的表面法向量，而且似乎直接用三角形法向量来着色更说得通。然而，在渲染时，三角形网格通常用来表示一个底层曲面，而点的法向量用来表示这个表面的朝向，而不是三角形网格本身。在16.3.4节，会讨论计算顶点法向量的方法。
顶点着色器是第一个处理三角形网格的第一个阶段。描述三角形组成的数据在顶点着色器这里并不可以获取。就如同它的名字表示的，它只处理进入的顶点。顶点着色器提供了一条途径，可以修改，和创建或忽略与每个三角形顶点相关的值，比如它的颜色，法向量，纹理坐标和位置。通常顶点着色器程序将顶点从模型客供件转换到齐次裁剪空间。至少，一个顶点着色器总要输出这个位置。
一个顶点着色器很像早前说的统一着色器。每个顶点进入，并由顶点着色器程序处理，然后输出一些值，插值于三角形或线中间。顶点着色器并不会创造或销毁顶点，而且一个顶点的结果并不会传递给另一个顶点。由于每个顶点都是独立处理的，GPU中任何数量的着色器处理器都可以对进入的顶点流进行并行处理。
输入汇编通常是在 顶点着色器前执行的一个过程。这是一个物理模型通常与逻辑模型不同的例子。物理上，获取数据并创建顶点也许会在顶点着色器中发生，驱动程序会静默地为每个着色器提前设置合适的指令，对开发者不可见。
之后的章节会解释几个顶点着色器的效果，比如在动画合并时的顶点融合，以及剪影渲染。其他顶点着色器的使用包括：
产生物体，通过创建一次网格，并用顶点着色器将其变形。
通过蒙皮和变形技术让角色的身体和脸动起来。
程序化变形，比如旗帜、布料和水的运动。
创建粒子，通过将退化的网格传入管线并根据需要得到一片区域。
镜头畸变，热雾，水波纹，卷页，和其他效果，通过将整个framebuffer的内容作为一个纹理，经过程序化变形，用在一个屏幕对齐的网格上。
用顶点纹理抓取应用地区高度域。
顶点着色器的输出可以用多种方式消耗掉。通常的路径是对于每个实例的原型，比如多个三角形，生成，并光栅化，然后像素碎片格子产生，送到像素着色程序继续处理。在某些GPU上，数据还会送到曲面细分阶段或几何着色阶段，或者存入内存中。在之后的章节会讨论这些可选阶段。

3.6 曲面细分阶段
曲面细分阶段让我们可以渲染曲面。GPU的任务是将每个表面的描述得到，并将其转换为一个三角形集合来表示。这个阶段是一个可选的GPU特性，第一次实在DirectX 11中可以使用（也是要求）。它也受到OpenGL 4.0和OpenGL ES 3.2的支持。
用曲面细分阶段有很多好处。曲面描述通常比直接提供对应的三角形要更小。除了节省内存，这个特性可以让CPU与GPU之间的交互不再是动画角色或每一帧都在变形的物体的瓶颈。物体表面可以被高效渲染，通过根据视角来决定一个合适的三角形数。例如，如果一个球在距离镜头较远的地方，只需要一部分三角形。近一些，也许用上千个三角形来表示球才能看上去效果最好。可以控制细节程度的能力也就可以哟过来控制性能，比如用低质量的玩个在比较弱的GPU上以保持帧率。模型通常用平面表示，这些平面可以转换为合适的三角形网格，然后按需要弯曲它们，或者它们可以被曲面细分以让昂贵的着色运算可以更少地执行。
曲面细分阶段总会包含三个元素。用DirectX的术语，它们分别是hull shader， tessellator和domain shader。在OpenGL中hull shader是tessellation control shader，而domain shader是tessellation evaluation shader，这个命名更加具有描述性，虽然冗长。固定功能的tessellator（曲面细分器）在OpenGL中叫做primitive generator(原型生成器)，而之后会讲到，这确实是它做的事情。
如何指定和划分曲线以及曲面会在第17章长篇讨论。这里我们只给出一个简短的每个曲面细分阶段的总结。一开始，输入到hull shader的是一个特殊的patch(补丁)原型。这包含了若干的控制点，可以用来定义一个细分曲面，贝塞尔补丁或其他类型的弯曲元素。hull shader有两个功能。第一个，它会告诉曲面细分器要产生多少个三角形，并以何种配置产生。第二，它会处理每个控制点。同时，可选的，hull shader可以修改接下来的patch描述，根据需要添加或者移除控制点。hull shader输出控制点集合，以及控制曲面细分的数据，给domain shader。
input patch -> hull shader 	--> tessellator ----|
							|				    |
							|-> domain shader <-|
hull shader --> tessellator: TFs(tessellation factors) & type
hull shader --> domain shader: control points, TFs & constants
tessellator --> domain shader: generated points
domain shader --> output mesh
在管线中，tessellator是一个固定功能的阶段，只用tessellation shader。它的任务是给domain shader添加若干新的顶点来处理。
hull shader发给tessellator需要的曲面细分类型：三角形，四边形或等高线。等高线是带状的线集合，有时用于头发的渲染。hull shader发送出的其他重要的值是曲面细分因子（tessellation factors，在OpenGL中叫做tessellation levels）。这些值有两个类型：内部和外部边界。两个内部因子决定了曲面细分在三角形或四边形内部的划分情况。外部因子决定了外部曲线如何分割。为了各自控制，我们可以让相邻的曲面边缘与曲面细分匹配，而不管其内部是如何划分的。让边缘匹配，避免裂纹或其他的在patches交界处的着色不良效果。顶点是用中心坐标分配的，即在想要的表面上，与指定的每个点的相对位置。
hull shader总会输出patch，一个控制点位置集合。
（个人理解，这里总算是找到了patch的合适解释，a set of control point locations，一个控制点位置集合）
然而，如果给tessellator发送的外部tessellation level是0或者更少（或者NaN），这个patch就会直接被忽略掉。其他情况下，tessellator会产生一个网格(mesh)，并将它发送到domain shader。来自hull shader的曲面控制点用于domain shader计算每个顶点的输出值的调用。domain shader有一个数据流模式就像vertex shader一样，每个从tessellator来的点都会被处理并产生一个对应的输出点。三角形如此形成，并传递到之后的管线。
这个系统听起来好复杂，之所以采用这样的结构是为了效率，而且每个shader都相当简单。patch传入hull shader常常几乎不加修饰。这个shader也许用patch中估计的距离或屏幕规模来在线计算曲面细分参数(TFs)，对区域渲染也一样。或者，hull shader也许会简单地传一些固定的值的集合给所有的应用计算和提供的patches。曲面细分会进行复杂的但是固定功能的过程以产生顶点，赋予它们位置，并指定它们构成的三角形或线。这个数据放大的步骤是在一个shader外运行的，这样做是为了提升计算效率。domain shader将每个点产生的重心坐标接下来，并用在patch的估值方程(evaluation equation)中来产生位置，法向量，贴图坐标以及其他需要的顶点信息。

3.7 几何着色器
几何着色器将原型变为其他原型，这是曲面细分阶段做不到的。比如，将三角形网格通过在每个三角形上连出边而转换为线框视图。或者，线可以被替换为面向观察者的四边形，这样就让线框渲染的边更粗。在2006年后期，DirectX 10发布，几何着色器被加入硬件加速的图形学管线。它被放在曲面细分着色器之后，而且它只是可选的。不过，它是Shader Model 4.0要求的一部分。OpenGL 3.2和OpenGL ES 3.2也支持这类shader。
几何着色器的输入是一个物体和与这个物体相关的顶点。物体通常包含在条带中的三角形，线段或者只有点。拓展的原型可以几何着色器定义和处理。特别地，三角形外的三个额外的点可以传入，折线中的两个相邻的点也可以使用。如果是DirectX 11，用的Shader Model 5.0，你可以传入更加复杂的patches，可以至多有32个控制点。这就意味着，曲面细分阶段在产生patch上更高效了。
几何着色器处理了这个原型，并输出不少于0个顶点，可以用于点、折线或是三角形带。注意，几何着色器不会生成任何输出。通过这种方式，一个网格可以被有选择地修改，通过编辑顶点，增加新的原型或移除一些部分。
几何着色器设计出来是为了修改输入除数，或进行有限数量的复制。比如，一个用法是产生一个数据的6个转换后的复制用来模拟一个立方体6个面的渲染情况，在10.4.3节提到。它也可以用于高效地产生级联的shadow map，用于高质量阴影的生成。其他算法也可以利用几何渲染器，包括用点数据创造可变规模的粒子，皮毛渲染中沿着轮廓挤压出fins，阴影算法中找到物体的边界。这些使用方式以及其他更多的使用方式会在这本书的剩余部分中讨论。在OpenGL 4.0中，这是明确了调用次数的。几何着色器也可以输出至多4个流。1个流可以送到下面的渲染管线用于更多处理。所有这些流可以有选择地送到流输出渲染目标。
几何着色器确保了原型的输出结果的顺序与输入的顺序相同。这与性能相关，因为多个着色器核在同时运行，而结果必须保存且排序。这个以及其他一些因素与阻止了几何着色器用于在一次调用中复制或创造大量的几何形状。
在draw调用发布后，GPU中只有3个地方的工作会别创建：光栅化，曲面细分阶段以及几何着色器。当然，考虑到用到的资源和内存，几何着色器的行为至少还是可以预测的，因为它是完全可编程的。在实践中，几何着色器用得很少，因为它并不能很好地契合GPU的能力。在一些移动设备上，它在一些软件中会用到，所以在那里并不鼓励使用它。

3.7.1 流输出
标准的GPU管线是将数据发到顶点着色器，然后光栅化三角形，并在像素着色器处理结果。过去的情况常常是，数据总会流经整个管线而中间的结果是无法获取的。在Shader Model 4.0中，流输出的理念被引入。在点着色器处理了顶点之后（曲面细分和几何着色器是可选的），这些结果可以在流中被输出，比如一个有序的数组，此外，可以将它送到光栅化阶段。光栅化阶段事实上可以被完全关掉，然后管线就可以被用作一个无图像的流处理器。
（个人理解，哦吼，这个好像跟课上讲的不太一样。GAMES101的课程上讲的依然是无法看到中间结果，因此中间调试是非常困难的。这也许可以用于途中的测试）
这种方式处理的数据可以重新送回管线，这样就可以迭代处理。这种操作在模拟流水的运动或其他粒子效果的时候是十分有用的，在13.8章节会讨论到。它也可以用于给模型蒙皮，然后将这些可以得到的顶点重用。
流输出只会以浮点数的形式返回数据，所以它有一个明显的内存代价。流输出用于原型，而不直接用于顶点。如果网格送到管线的下一环，每个三角形会产生它自己的三个输出顶点。原本网格中的所有共享顶点都会丢失。因此，一个更加经典的用法是只将顶点以点集原型的形式送入管线。在OpenGL中，流输出阶段叫做转换反馈(transform feedback)，因为它主要的用法是将顶点转换并返回它们用于之后进一步的处理。原型可以确保按输入顺序地送到流输出对象，这意味着点的顺序也会保留。

3.8 像素着色器
在顶点着色器，曲面细分着色器和几何着色器搞过之后，原型会被剪裁，并准备搞光栅化，如之前章节讲到的。管线的这部分在处理步骤中是相对固定的，比如不可编程但是某种程度上可配置。每个三角形都被遍历到以判定它覆盖了哪些像素。光栅化器也许只会粗略地计算每个三角形覆盖了多少像素单元面积（5.4.2章节）。由三角形部分或完全覆盖的像素片叫做碎片。
（终于找到fragment的定义了，This piece of a triangle partially or fully overlapping the pixel is called a fragment. 差不多，可以翻译成“像素碎片”）
三角形顶点的值，包括在z-buffer中的z值，在三角形表面的每个像素进行插值。这些值传递到像素着色器，也就是处理像素碎片的。在OpenGL中，像素着色器也叫做碎片着色器（fragment shader），这个名字也许更贴切。我们为了保持一致性，在这本书就都用"pixel shader"了。点和线的原型送入管线也会产生碎片，它们也会覆盖到像素。
三角形插值的种类由像素着色器的程序指定。通常我们用透视正确插值，这样世界空间中像素表面位置的距离会随着物体的远离而增加。一个例子就是渲染延伸到地平线的铁路。铁路在越远的地方靠的越近，因为水平方向上的相邻像素的世界空间距离越大。其他插值选项也是可以用的，比如屏幕空间插值，就是不考虑透视投影的插值。DirectX 11提供了更多的控制，可以决定什么时候，如何进行插值。
在编程方面，顶点着色器程序的输出，通过三角形（或线）插值，有效地变为了像素着色器的输入。随着GPU进化，其他输入也可以暴露。比如在Shader Model 3.0以及更高版本中，碎片的屏幕位置在像素着色器中也是可以获取的。同样，三角形中哪边是可见的是一个输入flag。这个知识对于在一轮中渲染三角形正反不同的材质是重要的。
有了输入，通常像素着色器会计算并输出一个碎片颜色。它可能还会产生一个不透明度的值，以及可选的修改z-depth。在合并阶段，这些值会改变每个像素中存储的值。在光栅化阶段产生的深度值也可以由像素着色器修改。模板(stencil)缓存中的值通常是不能改的，但是可以决定是否将它送到合并阶段。DirectX 11.3允许shader改变这个值。在SM4.0（Shader Model 4.0）后，诸如雾的运算和alpha测试这样的操作从合并操作中移到了像素着色运算中。
一个像素着色器还有其独特的能力：忽略一个输入的碎片，比如，不产生输出。（举个例子，切掉了物体的一部分，那么切掉部分的碎片就直接忽略）。切平面过去在固定功能管线中常常是可以配置的元素，之后就可以在顶点着色器中明确了。碎片可以忽略，那么前面说的这个功能就可以在像素着色器中实现，比如可以决定切掉的体积是取“与”还是“或”。
一开始，像素着色器的输出只可能是合并阶段，并最终显示出来。像素着色器可以执行的指令数量一直在大幅增长。这个增长催生了多渲染目标（multiple render targets, MRT）的想法。不向像素渲染器发送颜色和z-buffer，取而代之的是每个碎片可以产生多组值，并存入不同的buffers，每个都被称为render target。通常render target有着相同的x坐标和y坐标；一些API允许不同的规模，但是渲染的区域应当是这些可选规模中的最小值。一些结构要求render targets有相同的位深度，甚至特定的数据格式。取决于GPU，render targets的数量可以是4到8不等。
即使有这些限制，MRT功能仍然是高效渲染算法的强力支援。一个简单的渲染轮可以产生一个目标的颜色图，一个物体识别，一个世界空间距离。这个能力也让不同类型的渲染管线出现，叫做延迟着色（deferred shading），可见性和着色在不同的轮完成。第一轮存储每个像素的物体位置，材质数据。接下来的几轮可以有效地应用光照和其他效果。这种类型的渲染方法在第20.1节会讲解。
像素着色器的限制是它通常只会写入到render target给他的碎片位置，且不能读取当前邻近像素的结果。这意味着，当一个像素着色器程序运行，它不会直接将输出发到相邻的像素，也不会获取到它们的改动。它只会计算影响它自己像素的结果。不过这个限制也不像听起来那样严重。当前轮的输出结果可由之后轮获取。相邻像素的处理可以用到12.1章节中的图像处理技术。
像素着色器不能得知或影响其相邻像素的结果，这条规则是有原因的。一个是因为在计算梯度或导数时，像素着色器可以立即得到邻近碎片的信息（虽然是间接的）。像素着色器会得到每单位x值或单位y值插值的变化量。这些值在许多运算以及纹理定位中都有用。这些值在一些操作，如纹理过滤（6.2.2节，我们希望知道一个图片覆盖了一个像素的多少部分），是尤其重要的。所有现代的GPU通过将碎片按2 * 2的组（叫做quad，四元）处理，应用这个特性。当像素着色处理器需要一个梯度值时，邻近碎片的差值就会返回。一个统一的核有这个能力获取相邻的数据————让不同的线程在同一个warp————并计算出像素着色器需要的梯度值。这个应用的一个结果是，梯度值无法在一部分受动态分支流控制的shader中获取，比如"if"声明或迭代数为可变数量的循环。所有组内的碎片需要用同一组指令处理，因此四个像素的结果对于计算梯度来说是意义重大的。这甚至是离线渲染中也存在的基础的局限。
DirectX 11引入了一个缓存种类允许任何位置的写获取，乱序获取视图（unordered access view, UAV）。原本只是给像素着色器和计算着色器的，但是在DirectX 11.1中，UAV的获取拓展到了所有的着色器。OpenGL 4.3把这个叫做shader storage buffer object(SSBO，着色器存储缓存对象)。两个名字都在各自的意义上有一定的描述性。像素着色器并行地运行，以一个任意的顺序，而它们共享存储缓存。
通常，需要一些机制来避免，数据竞争（a.k.a，一种数据冒险，这算是编译原理中的概念了吧），两个着色器程序竞争影响同一个值，通常会产生一个随机的结果。例如，如果一个像素着色器的两个调用尝试同时加一个相同的查到的值，那么就可能出现错误。两者都会找到原始值，然后各自都会修改它，但是在写入时，后写入的会抹掉先写入的结果，实际就是只有一个加法实现了。GPU用专门的着色器可获取的原子单元来避免这个问题。不过，原子意味着一些着色器也许就需要停下来等待获取内存中的某个由另一个着色器正在进行读/改/写的位置。
原子性避免了数据冒险，而许多算法也需要特定的执行顺序。例如，你也许想要画一个远处的透明的蓝色的三角，然后再用一个红色的透明的三角覆盖它，将红色覆盖到蓝色上。用一个像素两个像素着色器调用，一个用于蓝色三角，一个用于红色三角，但可能发生这种情况，红色三角先完成，然后蓝色三角才完成。在标准的管线中，碎片结果在处理前是在合并阶段排序的。光栅化顺序视图（rasterizer order views, ROVs）在DirectX 11.3中引入，增强了执行的顺序。它与UAVs有些像：着色器的读和写的模式相同。但最大的不同在于，ROVs可以确保数据以合适的顺序被获取。这个特性让这些着色器可获取的缓存的实用性大幅提升。比如，ROVs让像素着色器写自己的融合方法成为了可能，因为它可以直接在ROV中直接获取和写，而不必需要合并阶段。代价是，如果一个失序的获取别检测到，那个这个像素着色器调用可能就会停止，一直等到约定顺序上前面的三角形处理都完成。

3.9 合并阶段
在2.5.2章节讨论过，合并阶段是每个碎片的深度和颜色（由像素着色器产生）和framebuffer结合的阶段。DirectX将这个阶段称为输出合并，OpenGL将其称为对每个样本的操作(per-sample operations)。在大部分传统的管线图（包括我们自己的），这个阶段是模板缓存和z-buffer操作发生的地方。如果一个碎片可见，另一个在这个阶段发生的操作就是颜色融合。对于不透明(opaque)的表面不会有真正的融合，只会发生碎片颜色替换掉之前存储的颜色。真正的碎片颜色融合和颜色存储，通常用于透明度和合成操作（5.5节）。
想象一下，一个光栅化生成的碎片通过像素着色器然后在z-buffer被考察时发现被之前渲染的某个碎片挡住了。那么所有在像素着色器中进行的处理就没有必要了。为了避免这种浪费，许多GPU在像素着器执行之前就进行了一些融合测试。这个功能叫做early-z。像素着色器可以改变改变碎片的z-depth或者忽略整个碎片，通常这是不需要的，也是关闭的，因为则通常会让管线效率更低。DirectX 11和OpenGL 4.2允许像素着色器强制early-z开启，虽然有很多限制。在23.7节，有更多关于early-z和其他z-buffer的优化。有效地运用early-z可以在性能上有很大效果，在18.4.5节会讨论这些细节。
合并阶段介于固定功能阶段，比如三角形建立阶段，和完全可编程阶段之间。虽然，它不是可编程的，但是它是高度可配置的。特别是颜色融合可以通过设置来进行大量不同的操作。最常用的是涉及到颜色和alpha值的乘法，加法和减法的组合，以及位运算操作。DirectX 10添加了将像素着色器的两个颜色和framebuffer颜色融合的能力。这个能力叫做双向源色融合(dual source-color blending)而且不能用于多个render target的结合。MRT确实某种意义上支持融合，DirectX 10.1引入了在每个独立buffer上进行不同融合的能力。
如之前章节结束时所说，DirectX 11.3提供了一种在ROVs中可编程融合的方法，虽然代价是性能降低。ROVs和合并阶段都确保了绘制顺序，又名，输出不变性。无论像素着色器产生结果的顺序如何，这是一个API要求，即结果必须是有序的，按输入的顺序发送给合并阶段，一个物体接一个物体，一个三角接一个三角。

3.10 计算着色器
GPU可以用的地方不只是传统的图形学管线。有许多无图像的使用方式，比如证券估值，和为深度学习训练神经网络。以这种方式使用硬件，叫做GPU计算。像CUDA和OpenGL平台可以用于将GPU当做一大堆并行处理器来控制，而不需要图形学具体的功能。这些框架通常会用C或C++作为拓展，以及一些针对GPU的库。
在DirectX 11中引入，计算着色器是一种GPU计算的形式，这个着色器不是图形学管线中的某个固定部分。它与图形学API渲染过程中的调用紧密相关。它和点、像素以及其他着色器不在一起。它基于统一着色器处理器池，与在管线中用到的一样。它是一个更其他着色器差不多的着色器，有一些输入数据集合，并能获取缓存（比如纹理）用于输入和输出。warps和线程在计算着色器中更可见。比如，每个调用可以获取到一个线程下标。同样，这里也有线程组的概念，在DirectX 11中，意味着1到1024个线程。这些线程组可以用x, y, z坐标指定，大多是为了着色器编码的简洁。每个线程组有少量的组内线程共享的内存。在DirectX 11中，这个数量是32kB。计算着色器由线程组执行，因此所有组内的线程会确保同步运行。
计算着色器的一个重要优势是，他们可以获取GPU产生的数据。将数据从GPU发到GPU会产生一个延迟，因此如果将处理和结果都放到GPU内，性能会提升。后期处理，即一个渲染好的图片以某种方式修改，在计算着色器中是很常见的。共享的内存意味着，从图片像素取样得到的中间结果可以在邻近的线程中共享。比如，用一个计算着色器来决定图片的平均光照或光照分布，比在像素着色器上速度快了1倍。
计算着色器在粒子系统、网格处理，比如面部动画，剔除，图片滤镜，提高深度精度，阴影，深度域和其他任何需要一组GPU处理器来承担的任务上都很有用。Wihlidal讨论了计算着色器为什么比tessellation hull shaders更加高效。
我们对GPU在渲染管线中的应用就在这里结束了。GPU的功能可以有很多种使用的方式，也可以组合出很多中渲染相关的处理。利用这些能力来对接相关的理论和算法是这本书的核心内容。我们现在可以将注意力放到转换和着色上。

更多相关阅读和资源
Giesen的图形学管线旅程详细讨论了GPU的许多方面，解释了为什么这些元素要那样工作。Fatahalian和Bryant的课程讨论了GPU并行，用到了一系列详细的幻灯片。至于用CUDA进行GPU计算，Kirk和Hwa的书介绍了这部分，并讨论了其进化过程和GPU的设计哲学。
要正式学习着色器编程需要一些努力。OpenGL Superbible和OpenGL Guide包含了着色器编程的材料。老书OpenGL Shading Language并没有提到最近的着色器阶段，比如几何着色器和曲面细分着色器，但是特别重点地讲了着色器相关的算法。看这本书的网站，realtimerendering.com，可以找到最近的推荐书籍。

第4章 转换
（由于这部分内容在GAMES101中讲过，练习过，且在《Fundamentals of Computer Graphics》(4th Edition)中看过，我只挑一些不熟悉的内容进行翻译和记录）
第一段大意：转换很重要，特别是对于计算机图形学。
线性变换：加法，常量乘法
仿射变换：可以表示线性变换 + 平移
为了将平移也用统一的形式实现，用到了齐次坐标。
仿射变换可以用一个4 * 4的矩阵表示（课堂上还额外讲到了，这个矩阵表示的是先线性变换，再平移，这是由推导过程决定的）
3D的点：(x, y, z, 1)
3D的向量：(x, y, z, 0)
一般地，(x, y, z, w)(w != 0)，表示(x / w, y / w, z / w)的三维的点
仿射变换的特点：变换后线仍然是平行的
正交矩阵，它的逆矩阵是它的转置（线性代数基础知识，不过这里的正交矩阵还有正交投影的含义，而不仅仅是数学意义上的正交）

4.1 基础变换
平移、旋转、缩放、剪切、欧拉、正交、投影、斯勒普变换
欧拉变换：欧拉角，左右摆动、俯仰、滚动。正交 + 仿射变换
斯勒普变换：四元数的插值，两个四元数 和 一个参数t。查到的一种用法是：球形插值。

4.1.1 平移变换
T = [[1, 0, 0, tx], [0, 1, 0, ty], [0, 0, 1, tz], [0, 0, 0, 1]]

4.1.2 旋转变换
绕坐标轴的变换：
Rx(φ) = [[1, 0, 0, 0], [0, 0, cosφ, -sinφ, 0], [0, sinφ, cosφ, 0], [0, 0, 0, 1]]
Ry(φ) = [[cosφ, 0, sinφ, 0], [0, 1, 0, 0], [-sinφ, 0, cosφ, 0], [0, 0, 0, 1]]
Rz(φ) = [[cosφ, -sinφ, 0, 0], [sinφ, cosφ, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]
举例，绕一个点的变换。如果我们想让一个物体绕z轴旋转φ角度，而且旋转的中心是点p，如何做到？
将物体平移，使得p点与原点重合，然后旋转，然后再移回原位置。
公式，X = T(p)Rz(φ)T(-p)，注意变换的顺序，-p是移到与原点重合，p是移回去。

4.1.3 缩放变换
设S(s) = S(sx, sy, sz)，sx, sy, sz表示在三个方向上的缩放
S(s) = [[sx, 0, 0, 0], [0, sy, 0, 0], [0, 0, sz, 0], [0, 0, 0, 1]]
S_inverse(s) = S(1/sx, 1/sy, 1/sz) 
如果用齐次坐标系，另一种实现缩放的方式是直接改w的值，不过这么可能会影响效率，因为需要做除法。
镜像矩阵：缩放系数是负的。注意，镜像矩阵接旋转矩阵仍然是一种镜像矩阵。
镜像矩阵需要特别注意，因为三角形逆时针排列的顶点经过镜像会变成顺时针，这可以引起光照错误和背面剔除。要检测是否有镜像变换在其中，可以计算矩阵的行列式值，如果是负的，那么有镜像发生。

举例，对某个方向的缩放。做法：将其分解到三个坐标轴方向上。公式，X = FS(x)F_transpose，其中F = [[fx, fy, fz, 0], [0, 0, 0, 1]]，fx, fy, fz为正交的单位化的，正确朝向的向量。

4.1.4 剪切变换
常用于产生迷幻视觉效果，扭曲物体外观
Hxz(s) = [[1, 0, s, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]，技巧，x表示行下标，z表示列下标，所以s对应的是矩阵中下标为(0, 2)的元素。
逆变换，Hij_inverse(s) = Hij(-s)
剪切变换的一个变体H'xy(s, t) = [[1, 0, s, 0], [0, 1, t, 0], [0, 0, 1, 0], [0, 0, 0, 1]]，实际上相当于Hxz(s)Hyz(t)。
所以，H'ij(s, t) = Hik(s)Hjk(t)
注意，所有剪切变换矩阵的行列式值为1，不改变体积。

4.1.5 变换组合
注意，变换的组合是有顺序性的
比如TRS(p)是先进行S，再进行R，最后进行T的。不过，在实际计算中，我们可以先将TR矩阵合并计算出一个结果，再与S进行计算。

4.1.6 刚体变换
只有位置和朝向发生变换：平移 + 转动
X = T(t)R = [[r00, r01, r02, tx], [r10, r11, r12, ty], [r20, r21, r22, tz], [0, 0, 0, 1]]
X_inverse = R_inverse * T_inverse(t) = R_transpose * T_inverse(t)
例如：调整镜头朝向
假设我们的镜头位置在c，我们希望让镜头看向目标l，而镜头的初始向上方向为u'。我们希望计算出三个新的基准{r, u, v}
v = (c - l) / ||c - l||，r = -(v X u') / ||v X u'||（这里X表示叉乘，也可以根据情况用v.crossProduct(u')来表示），u = v X r
得到基准变换：A = [[rx, ry, rz, 0], [ux, uy, uz, 0], [vx, vy, vz, 0], [0, 0, 0, 1]]
但在基准变换前，我们应该先将镜头平移，因此平移变换：T(-t) = [[1, 0, 0, -tx], [0, 1, 0, -ty], [0, 0, 1, -tz], [0, 0, 0, 1]]
综上，得到镜头变换 M = AT(-t)

4.1.7 法向量变换
经典做法是需要计算矩阵的逆的转置，而计算矩阵的逆需要计算矩阵的伴随矩阵。不过计算一个4 * 4矩阵的伴随矩阵的代价是昂贵的。由于实际场景中的变换一般不改变w值，因此可以计算左上3 * 3部分的伴随矩阵。或者，有时连伴随矩阵都不用计算，只需要将影响到法向量的变换的逆变换应用一下。比如，旋转变换改变了法向量，那就转回去。

4.1.8 矩阵的逆的计算
方法1：用一个新的变换抵消原来的变换，作为原来变换的逆变换。
方法2：对于正交矩阵，矩阵的逆 = 矩阵的转置
方法3：伴随矩阵，克拉默法则，LU分解或高斯消元都可以用来计算逆矩阵。通常伴随矩阵和克拉默法则是比较常用到的，因为它们的分支操作更少（个人理解，结合之前的内容，这样的运算在GPU上表现更好）。

4.2 特殊变换和运算
4.2.1 欧拉变换
E(h, p, r) = Rz(r)Rx(p)Ry(h)
E_inverse = E_transpose = (RzRxRy)_transpose = Ry_transpose * Rx_transpose * Rz_transpose，直接用E_transpose更简洁
注意，z轴方向，大部分人工处理过程，包括3D打印，认为z轴正向为向上的方向；但是在航海和航空中，将z轴负向认为是向上的方向。
建筑和地理信息系统通常用z轴正向。媒体相关的建模系统常常将y轴当做世界坐标的向上方向，这也是为什么我们总要在计算机图形学中描述镜头的向上方向。两个世界坐标系间的不同可以通过一个90°的旋转（或者镜像）完成，但如果不知道哪一个是默认的就会导致问题。在本章节中，我们采用y轴正向的世界方向。
镜头的向上方向与世界的向上方向没有什么特殊关联。
欧拉角有很多限制。在涉及到两套欧拉角时，工作就会很困难。

4.2.2 欧拉变换矩阵的参数
head(绕y轴), pitch(绕x轴), roll(绕-z轴)
E(h, p, r) = [[e00, e01, e02], [e10, e11, e12], [e20, e21, e22]]
之所以这里用3 * 3的矩阵，是因为这注意提供旋转矩阵的所有必要信息。
e00 = cos(r) * cos(h) - sin(r) * sin(p) * sin(h)
e01 = -sin(r) * cos(p)
e02 = cos(r) * sin(h) + sin(r) * sin(p) * cos(h)
e10 = sin(r) * cos(h) + cos(r) * sin(p) * sin(h)
e11 = cos(r) * cos(p)
e12 = sin(r) * sin(h) - cos(r) * sin(p) * cos(h)
e20 = -cos(p) * sin(h)
e21 = sin(p)
e22 = cos(p) * cos(h)
由此，我们可以得到head和roll参数：e01 / e11 = -tan(r), e20 / e22 = -tan(h)
于是有，h = arctan(-e20 / e22), p = arcsin(e21), r = arctan(-e01 / e11)
一种特殊情况，如果cos(p) = 0，我们得到的是gimbal lock(云台锁，也叫做万向锁，三维中一个维度的自由度丧失)：
E = [[cos(r), sin(r)cos(p), sin(r)sin(p)], [sin(r), cos(r)cos(p), -cos(r)sin(p)], [0, sin(p), cos(p)]]
举例，限制转换，用这个章节的方法，可以限制转换，比如限制只能绕x轴旋转。
 
4.2.3 矩阵分解
对一个复合的矩阵追溯其中的多种变换，叫做矩阵分解。
比如：提取出缩放系数，找出特定系统需要的转换，判断模型是否只有刚体变换，在动画关键帧中插值而只有物体的矩阵是可以获取的，从一个旋转矩阵中移除剪影。
对于这个话题，有很多文章，线上也有很多可用的代码。Thomas和Golldman各自展示了对多种变换的不同方法。Shoemake提升了他们仿射矩阵的技术，因为他的技术不依赖于帧，且将矩阵分解以获取刚体变换。

4.2.4 绕任意轴的旋转
我们可以先以这个任意轴r为一个轴建立一个正交的三维坐标轴，然后将这个新的三维坐标轴旋转到标准的x, y, z轴上，并让x轴与r轴重合，绕x轴旋转，再将新三维坐标轴转回去。
那么在已知一个坐标轴r的情况下，如何建立一个正交三维坐标。可以采用如下的建立方法：
1)如果|rx| <= |ry| 且 |rx| <= |rz|， s = (0, -rz, ry)
2)如果|ry| <= |rx| 且 |ry| <= |rz|， s = (-rz, 0, rx)
3)如果|rz| <= |rx| 且 |rz| <= |ry|， s = (-ry, rx, 0)
然后，将s单位化s /= ||s||
并求出另一个正交轴t = r X s
M = [[rx, ry, rz], [sx, sy, sz], [tx, ty, tz]]
由于M是正交矩阵，M_inverse = M_transpose
所以变换矩阵 X = M_transpose * Rx(α) * M
另一种方法是Goldman提出的，这里直接写结论：
R = [[cosφ + (1 - cosφ) * rx * rx, (1 - cosφ) * rx * ry - rz * sinφ, (1 - cosφ) * rx * rz + ry * sinφ],
	 [(1 - cosφ) * rx * ry + rz * sinφ, cosφ + (1 - cosφ) * ry * ry, (1 - cosφ) * ry * rz - rx * sinφ],
	 [(1 - cosφ) * rx * rz - ry * sinφ, (1 - cosφ) * ry * rz + rx * sinφ, cosφ + (1 - cosφ) * rz * rz]]
在4.3.2节，还有一种用到四元数的方法解决这个问题。

4.3 四元数

4.3.1 数学背景知识
每个四元数用4个实数表示，每个实数和一个不同的部分相关。
q = (qv, qw) = iqx + jqy + kqz + qw = qv + qw，qv为向量
qv = iqx + jqy + kqz = (qx, qy, qz)
i * i = j * j = k * k = -1
j * k = -k * j = i, k * i = -i * k = j, i * j = -j * i = k
对于虚部(imaginary part)qv，i, j, k叫做虚单元(imaginary units)
两个四元数的乘法：q * r = (iqx + jqy + kqz + qw) * (irx + jry + krz + rw) = (qv X rv + rw * qv + qw * rv, qw * rw - qv · rv)，其中X表示叉乘，· 表示点乘，*表示标量乘（注意区分，之后的共轭也会用*，但是*后没有别的字符）
加法：q + r = (qv + rv, qw + rw)
共轭：q* = (qv, qw)* = (-qv, qw)
大小（规范化）：n(q) = sqrt(qq*) = sqrt(qv·qv + qw * qw) = sqrt(qx * qx + qy * qy + qz * qz + qw * qw)
Identity：i = (0, 1)
n(q) * n(q) = qq*，∴qq* / n(q)^2 = 1
逆：q_inverse = 1 / n(q)^2 * (q*)

共轭律：(q*)* = q，(q + r)* = q* + r*，(qr)* = (r*) (q*)
规范律：n(q*) = n(q)，n(qr) = n(q)n(r)
乘法规则：
线性规则：
p(sq + tr) = spq + tpr，其中p, q, r为四元数，s, t为标量
(sp + tq)r = spr + tqr，其中p, q, r为四元数，s, t为标量
结合律：
p(qr) = (pq)r，其中p, q, r为四元数

如果n(q) = 1，那么q = (qv, qw)称为单位四元数，可以写作：q = (sinφ uq, cosφ) = sinφ uq + cosφ
uq为三元向量，且||uq|| = 1
对于复数，即一个二维的单位向量，可以写作：cosφ + i * sinφ = e^(iφ)
那么对应地，四元数q = sinφ uq + cosφ = e^(φ uq)
对数：log(q) = φ uq
乘方：q^t = sin(φt)uq + cos(φt)

4.3.2 四元数转换
单位四元数的一个最重要的特性是可以用来表示任意三维旋转，且这种表示非常的简洁。
可以证明：qpq_inverse，可以表示对轴uq将p旋转了2φ角度，其中p = (px, py, pz, pw)，是一个向量或者点，q = (sinφ * uq, cosφ)，是一个单位四元数。
由于q是单位四元数所以q_inverse = q*，即q的逆 = q的共轭
且对q的任意非0数倍表示的变换是相同的，则意味着q 和 -q表示的旋转是相同的，即将uq变为负向，让qw变为相反数，变换与原本的变换是一样的。
而如果有两个变换q, r，那么两者的结合可以表示为：r(qpq*)r* = (rq)p(rq)* = cpc*
c = rq，表示q, r变换的复合
矩阵形式：
我们将qpq_inverse，即qpq*，表示为矩阵形式
Mq = [[1 - s * (qy * qy + qz * qz), s * (qx * qy - qw * qz), s * (qx * qz + qw * qy), 0],
	  [s * (qx * qy + qw * qz), 1 - s * (qx * qx + qz * qz), s * (qy * qz - qw * qx), 0],
	  [s * (qx * qz - qw * qy), s * (qy * qz + qw * qx)]   , 1 - s * (qx * qx + qy * qy)], 
	  [0,                     , 0                          , 0,                       1]]
其中，s = 2 / (n(q))^2
对于单位四元数，矩阵可以简化为：
Mq = [[1 - 2 * (qy * qy + qz * qz), 2 * (qx * qy - qw * qz), 2 * (qx * qz + qw * qy), 0],
	  [2 * (qx * qy + qw * qz), 1 - 2 * (qx * qx + qz * qz), 2 * (qy * qz - qw * qx), 0],
	  [2 * (qx * qz - qw * qy), 2 * (qy * qz + qw * qx)]   , 1 - 2 * (qx * qx + qy * qy)], 
	  [0,                     , 0                          , 0,                       1]]
用了四元数，就不需要计算三角函数了。在实践中，这个转换过程是高效的。
从Mq得到四元q的过程：
先列出方程：
m21 - m12 = 4 * qw * qx
m02 - m20 = 4 * qw * qy
m10 - m01 = 4 * qw * qz
tr(Mq) = 4 * qw^2 / (n(q))^2

对于单位四元数，结果如下：
qw = 1 / 2 * sqrt(tr(Mq))
qx = (m21 - m12) / 4 * qw
qy = (m02 - m20) / 4 * qw
qz = (m10 - m01) / 4 * qw
为了避免除法，我们可以定义t = qw^2 - qx^2 - qy^2 - qz^2，
那么，
m00 = t + 2 * qx^2, m11 = t + 2 * qy^2, m22 = t + 2 * qz^2
u = m00 + m11 + m22 = t + 2 * qw^2
所以，
4 * qx^2 = + m00 - m11 - m22 + m33
4 * qy^2 = - m00 + m11 - m22 + m33
4 * qz^2 = - m00 - m11 + m22 + m33
4 * qw^2 = tr(Mq)
可以通过计算其中一个最大的值，得到qx, qy, qz中的最大者，然后用这个值结合之前的方程得到其余的量。

球体线性插值
给定两个单位四元数q, r，和一个参数t，t∈[0, 1]，计算出插值四元数
线性代数的形式表示如下：
s(q, r, t) = (rq_inverse)_transpose q
在软件中，表示为：
s(q, r, t) = slerp(q, r, t) = sin(φ(1 - t)) / sin(φ) * q + sin(φt) / sin(φ) * r，其中cosφ = qx * rx + qy * ry + qz * rz + qw * rw
slerp函数得到的结果是，在单位球表面q(t = 0), r(t = 1)之间的最短弧上t分点说代表的值。

如果有多个需要插值的区间，比如q0, q1, q2, ... , qn-1。可以用slerp直接插值，但是
另一种更好的插值方法是用样条插值。
引入ai和ai+1，介于qi与qi+1之间
ai = qi * exp(-(log(qi_inverse * qi-1) + log(qi_inverse * qi+1)) / 4)
于是有squad = (qi, qi+1, ai, ai+1, t) = slerp(slerp(qi, qi+1, t), slerp(ai, ai+1, t), 2t(1 - t))

从一个向量旋转到另一个向量
假设我们想从s方向转到t方向，且中间的路径尽量短，做法如下：
1)单位化s和t，计算出单位旋转轴u。u = (s X t) / ||s X t||
2)e = s · t = cos(2φ), ||s X t|| = sin(2φ)，2φ是s与t之间的角度
3)q = (sinφ u, cosφ)，或者用如下公式：
q = (qv, qw) = (1 / sqrt(2 * (1 + e)) * (s X t), sqrt(2 * (1 + e)) / 2))
旋转矩阵如下：
R(s, t) = [[e + h*vx*vx, h*vx*vy - vz, h*vx*vz + vy, 0],
		   [h*vx*vy + vz, e + h*vy*vy, h*vy*vz - vx, 0],
		   [h*vx*vz - vy, h*vy*vz + vx, e + h*vz*vz, 0],
		   [0, 0, 0, 1]]
其中v = s X t，e = cos(2φ) = s · t，h = (1 - cos(2φ) / sin(2φ)^2) = (1 - e) / (v · v) = 1 / (1 + e)
注意，如果s和t接近平行，那么φ将接近于0，我们可以直接返回单位矩阵。

4.4 顶点融合
（个人理解，将顶点周围所有对其有影响的顶点的影响按不同权重叠加到该顶点，得到该顶点的变换，为顶点融合）
比如，动画中，胳膊弯曲时，在胳膊肘部顶点的变换。
公式：
u(t) = ∑wi*Bi(t)*Mi_inverse*p，其中∑wi = 1，wi >= 0
Mi是将骨骼坐标转换到世界坐标的矩阵，Mi_inverse为其逆转换。
Bi(t)是在物体运动时骨骼i自身的变换。

4.5 变形
从一个模型变到另一个模型
变形主要要解决两个问题：顶点对应问题和插值问题
线性插值：m = (1 - s)p0 + sp1，p0和p1是不同时刻的同一个点

4.6 几何缓存回放
为了节省内存，用到了很多方法。
比如，量化（用16位整数存储坐标），时间、空间预测。
比如，如果上一帧到这一帧某个点变化了δ，那么这一帧到下一帧这个点也很有可能变化δ。

4.7 投影变换
正交投影、透视投影
这部分内容在GAMES101中进行了详细讲解。我只记录一下可能没提到的内容。
正交投影：
在OpenGL的规范立方体是(-1, -1, -1)到(1, 1, 1)，而DirectX是(-1, -1, 0)到(1, 1, 1)
我们设这两个边界点为(l, b, n)和(r, t, f)，因为我们看向的是-z方向，n > f
P = [[2 / (r - l), 0, 0, 0], [0, 2 / (t - b), 0, 0], [0, 0, 2 / (f - n), 0], [0, 0, 0, 1]] * [[1, 0, 0, -(l + r) / 2], [0, 1, 0, -(t + b) / 2], [0, 0, 1, -(f + n) / 2], [0, 0, 0, 1]]
对于右手系和左手系，从右手系变换到左手系还相当于进行一次镜像
P = [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]
DirectX用的是[0, 1]而不是OpenGL的[-1, 1]，那么正交投影矩阵变为了：
P = [[2 / (r - l), 0, 0, -(r + l) / (r - l)], [0, 2 / (t - b), 0, -(t + b) / (t - b)], [0, 0, 1 / (f - n), -n / (f - n)], [0, 0, 0, 1]]，而且由于DirectX用的是行为主形式来写矩阵，常常会用这个矩阵的转置。
透视投影：
P = [[2n / (r - l), 0, -(r + l) / (r - l), 0], [0, 2n / (t - b), -(t + b) / (t - b), 0], [0, 0, (f + n) / (f - n), -2f * n / (f - n)], [0, 0, 1, 0]]
超出边界，被截断的物体的变换矩阵：
P = [[2n / (r - l), 0, -(r + l) / (r - l), 0], [0, 2n / (t - b), -(t + b) / (t - b), 0], [0, 0, 1, -2n], [0, 0, 1, 0]]
在OpenGL中，先乘以S(1, 1, -1, 1)，这样变换后的0 < n' < f'
P = [[2n' / (r - l), 0, (r + l) / (r - l), 0], [0, 2n' / (t - b), (t + b) / (t - b), 0], [0, 0, -(f' + n') / (f' - n'), -2f' * n' / (f' - n')], [0, 0, -1, 0]]
在DirectX中，z是[0, 1]的，且看向z轴正向，变换矩阵如下：
P = [[2n' / (r - l), 0, -(r + l) / (r - l), 0], [0, 2n' / (t - b), -(t + b) / (t - b), 0], [0, 0, f' / (f' - n'), -f' * n' / (f' - n')], [0, 0, 1, 0]]

对于OpenGL投影变换，
zNDC = d - e / pz，其中 d = -(f' + n') / (f' - n')，e = -2f' * n' / (f' - n')，vx = -pz
NDC表示normalized device coordinates，标准化设备坐标，zNDC ∈ [−1,+1]
为了提高精度，我们可以存储1.0 - zNDC，用浮点数。
提高shadow maps的精度，可以用深度值的对数。
z = w(log2(max(1e-6, 1 + w)) * fc - 1)，用于OpenGL
z = wlog2(max(1e-6, 1 + w)) * fc / 2，用于DirectX
fc = 2 / log2(f + 1)

第5章
着色基础
当你渲染三维模型的图片时，模型不只是有合适的几何形状，他们还应该有理想的视觉外观。与应用相关，它可以从写实主义(photorealism)——与真实物体的照片相近的外观，到各种因为创作原因而风格化的外观。
本章节将要讨论到可以应用于写实和风格化渲染共同的那些方面。第15章将重点讨论风格化渲染，也是这本书的一个重要部分。第9到14章将集中讨论基于物理的方法，常用于写实渲染。

5.1 着色模型
选择一个着色模型来描述基于一些因素，比如表面的朝向，视角和光照，物体的颜色是如何变化的，这将是决定渲染物体外观的第一步。
举个例子，我们用一个Gooch着色模型的变体。这是一种非真实的渲染，也是第15章的内容。Gooch着色模型用于通过技术的表示方法提升细节的易读性。
Gooch模型背后的想法是将表面法向量与光的位置比较。如果法向量指向光源，那么表面的颜色更偏向暖色调；如果它背向光源，那么就用偏冷的颜色。中间的角度就用两极色调的插值，这个插值和用户提供的表面颜色有关。在这个例子中，我们给模型加入了风格化的高光效果，让表面呈现一种闪亮的视觉效果（个人感受，看起来有点油腻）。
着色模型通常有一些参数，用于控制外观的变化。设置这些参数的值将进一步决定物体的外观。我们举例的模型，只有一个参数，即表面的颜色。
就像大多数的着色模型，例子中的模型受到相对于视觉方向和光照方向的表面朝向影响。为了着色的目的，这些方向通常用单位化的向量。
现在我们定义了着色模型的所有输入，我们可以看一下模型的数学定义：
c_shaded = s * c_highlight + (1 - s) * (t * c_warm + (1 - t) * c_cool)
在这个等式中，中间步骤如下：
c_cool = (0, 0, 0.55) + 0.25 * c_surface
c_warm = (0.3, 0.3, 0) + 0.25 * c_surface
c_highlight = (1, 1, 1)
t = ((n·l) + 1) / 2
r = 2*(n·l)*n - l
s = (100*(r·v) - 97) （clamp[0,1]）
其中n, l为向量，n为表面法向量，l为光线方向，s取值取到[0, 1]区间内。
这种用方向向量点乘求夹角的余弦和线性插值在着色模型中非常常见。由于太常用，线性插值一般都是直接由着色语言内置实现的，一般函数名叫做lerp(linear interpolation)或mix。
r = 2*(n·l)*n - l计算的是反射光向量。l关于n的反射。一般内置实现的函数名叫做reflect。
通过将这样的运算用不同的方式结合，采用不同的数学表达和着色参数，着色模型可以定义为很多种风格和写实外观。

5.2 光源
在我们的例子中，光照很简单，光源对物体的影响也比较单一。在现实世界中，光照可以很复杂，可以有多个光源且每个都有自己的大小、形状、颜色和强度，间接光照加入了更多的变化。我们将在第9章看到这些，基于物理的，写实的光照模型，需要将所有这些参数都进行考虑。
相反，风格化的着色模型可能会对光照有很多不同的用法，取决于我们应用的需要和视觉的风格。一些高度风格化的模型可能都没有光照的概念，或者只用它来提供一定的方向性（比如Gooch渲染的例子）。
对于着色模型的光照复杂度，下一步需要考虑的是是否有光。这种模型的着色表面在有光照到的时候是一个外观，而在不受光影响的时候是另一个外观。这表明这两种情况需要一些边界条件来区分：与光源的距离，阴影（在第7章中会讨论），表面是否背向光源，或者一些其他的这些因素的组合。
从是否有光再稍微进一步就是光的强度的区间。这可以表述为有光和没有光之间的简单插值。如果采用无界的数，一个常见的例子是在着色模型有光和没有光的部分的因子，光的强度k_light用来衡量有光的部分。
c_shaded = f_unlit(n, v) + k_light * f_lit(l, n, v)
这个可以轻松拓展到RGB颜色c_light
c_shaded = f_unlit(n, v) + c_light * f_lit(l, n, v)
对于多个光源：
c_shaded = f_unlit(n, v) + Σc_lighti * f_lit(li, n, v)
没有点亮的部分f_unlit(n, v)对应“不受光影响的外观”，即模型将光只视作有或无两种状态。它可以有很多中形式，取决于想要的视觉风格和应用的需求。比如f_unlit() = (0, 0, 0)可以让任何不受到光影响的表面变为黑色。或者，无光的部分可以表现某种形式的风格化外观，类似于Gooch模型中背光的表面用冷色调。通常，着色模型的这部分表达出有一些不直接来自明确光源的光照，比如天空的光或者周围物体弹射过来的光。这些其他形式的光会在第10章和第11章中讨论。
我们之前提到了在物体表面法向量与光的方向成大于90度的角时，光源并不影响物体表面，实际上，这是表面下面的。这可以视作是一个更一般的光的方向与表面的关系、对着色的效果中的特例。虽然基于物理，这种关系可以追溯到一个简单的几何定理且在许多不基于物理的风格化的渲染模型中有用。
光对表面的效果可以看作是一组射线，射线打到表面的密度对应光在表面渲染中的强度。
（个人理解，这部分内容在GAMES101中提到了，且在GAMES202中回顾了，大意就是光的方向与表面法向的角度的余弦是实际有效的光，类比“有效面积”的概念。这也解释了为什么夏天热冬天凉）
如果点乘的结果是负值，说明光线来自表面背后，就没有效果。因此，我们对点乘的结果需要取到区间[0, 1]内的值。
所以，如果对点乘结果取[0, 1]区间内的值，我们可以将上面的方程稍微修改一下：
c_shaded = f_unlit(n, v) + Σ(li, n)_+ * c_lighti * flit(li, n, v)
一些风格化的模型也借用了这个方程以确保光照在全局的一致性，不过有的模型对这个结构的支持并不好，比如用这个公式的退化版，不取[0, 1]区间内值的那个。
最简单的方法是，将f_lit()取值为常数。f_lit() = c_surface
这就让着色模型简化为：
c_shaded = f_unlit(n, v) + Σ(li, n)_+ * c_lighti * c_surface
这个模型中点亮的部分对应Lambertian着色模型，Johann Heinrich Lambert在1760年发明了它！这个模型是在理想散射表面的前提下可行的，比如表面是完美哑光的。我们在这里展示的是简化过的Lambert模型的解释，在第9章会有更为严格的描述。Lambertian模型可以用于简单着色，在许多着色模型中它是一个重要的组成部分。
从上面的方程中，我们可以发现，光源与着色模型通过两个参数进行交互：l，指向光的方向；c_light，光的颜色。有许多种不同类型的光源，其本质区别在于这两个参数是如何在场景中变化的。
我们接着将讲几个流行的光源类型，它们有一个共同点：给定一个物体表面位置，每个光源都从一个方向照亮它。换言之，光源，从着色表面位置看，是一个无限小的点。在真实世界的光源其实并不是这样，不过大多光源的尺度和它们与物体表面的距离的尺度相比都小得多，因此这个近似也是合理的。在章节7.1.2和10.1，我们会讨论从一个范围内的方向中照亮表面位置的光源，比如区域光源(area lights)

5.2.1 定向光照
定向光照是光源模型中最简单的一种。l和c_light在场景中都是不变的，也许c_light可能因为阴影会变弱。定向光源没有空间中的位置，是抽象的。
对定向光照的某种拓展是允许c_light值的改变而光的方向l仍然是不变的。这常用于为了性能或创作原因将光的效果限制在场景中的特定区域。

5.2.2 准确光照
punctual light不是准时的光，而是有一个具体位置的光，不像定向光照。这种光照同样没有直径，没有形状或大小，并不像真实世界的光源。此处用"punctual"实际上是在拉丁文中"punctus"，意思是点，即源于单个位置的光源类型。我们用“点光源”来表示特定的一种发射器，即在所有方向上发出等量的光的发射器。所以点和点光源是两种不同形式的准确光照。光照方向向量与当前着色物体表面点的位置有关。
l = (p_light - p_o) / ||p_light - p_o||

点光源(Point/Omni Lights)
准确光照中对所有方向均等放出光线的被称为点光源或全向光源。对于点光源，c_light是随距离r变化的函数。
c_light(r) = c_light0 * (r0 / r)^2
当r接近0的时候，就可能出现除以0的情况，一般的做法是加一个小量ǫ来作为除数：
c_light(r) = c_light0 * (r0^2 / (r^2 + ǫ))
在虚幻引擎中，ǫ的取值为1cm
另一种修改的方式，在CryEngine和Frostbite游戏引擎中，是将r限制在一个有下限的区间中，下限值为r_min：
c_light = c_light0 / (r0 / max(r, r_min))^2
另一种情况，在距离比较大的手，可以将c_light视作0。
用一个window function乘以原方程，可以让整个函数的值在一个窗口区间内平稳变化。
虚幻引擎和Frostbite游戏引擎中用的窗口函数是：f_win(r) = (1 - (r / r_max)^4)，其中结果取值最小取到0。
有时为了满足性能要求，采用了更简化的方程：c_light(r) = c_light0 * f_dist(r)，f_dist(r) = (1 - (r / r_max)^2)

Spotlights
不像点光源，几乎所有真实世界的光源都会在方向、距离发生变化时变化。这种变化可以表示为一个方向性的跌落函数f_dir(l)。
c_light = c_light0 * f_dist(r) * f_dir(l)
f_dir（l）的选择不同会产生不同的光照效果。一种重要的光照效果是聚光灯效果，即将光投射到成一个圆锥。这个方向跌落函数关于聚光灯方向向量s具有旋转对称性，因此可以被表示为光到表面的方向l与s的夹角θs的函数。这个角度的最大值为θu(umbra angle，本影角)，而有全光强的角度称为半影角θp(penumbra angle)。θs为-l和s的夹角。
t = (cosθs - cosθu) / (cosθp - cosθu)，t取值取到[0, 1]。
聚光灯的方向跌落函数大多是相似的，在Frostbite游戏引擎中函数f_dirF(l) = t^2，在three.js浏览器图形库中函数f_dirT(l) = smoothstep(t) = t^2 * (3 * t - 2)

5.2.3 其他光照类型
定向光照和准确光照可以由光照方向如何计算来区分。用别的方法来计算光照方向可以定义其他类型的光照。比如《古墓丽影》中将光照封装为一个线段而不是一个点。对于每个着色像素，线段上到该像素最近的点被用作决定光的方向l。
目前讨论的光照类型都是抽象的。在现实世界中，光源是有大小和形状的，它们可以从多个方向照亮表面的点。在渲染中，这种光被叫做面积光，而它们在实时应用中的使用在持续增长。

5.3 着色模型的应用
讲点应用，代码方面的简单例子。

5.3.1 评估频率
在设计一个着色应用时，计算需要根据它们的评估频率被拆分。首先，判断一个计算结果是不是在整个绘制调用中都是常量。这种情况下，计算通常由应用完成，一般在CPU上，虽然GPU计算着色可以用于特别开销大的计算。结果通过统一的着色器输入传给图形API。
（接下来是对评估频率的举例，只做一次的，做多次但是变化很缓慢的，每帧都进行的，或每个模型都进行的）
将着色器输入按照评估的频率来分组是有益于提升效率的，也可以通过最小化更新量来提升提升GPU性能表现。
理论上，着色计算可以在任何可编程阶段进行，其中每个对应一个不同的评估频率：
点着色：在每个曲面细分前的点上评估
曲面细分着色器：在每个表面补丁上评估
主体着色器：对每个曲面细分后的点进行评估
几何着色器：对每个原型进行评估
像素着色器：对每个像素进行评估
在实践中，大部分的着色计算是在每个像素上进行的。通常，这些计算在像素着色器中进行，计算着色器的应用也越来越普遍，在第20章讨论了多个例子。其他阶段主要用于几何运算比如变换和形变。要理解为什么这样，我们需要比较每个点和每个像素着色的评估。在过去的教材中，有时也将其称为Gouraud shading和Phong shading，不过这些术语今天不太用了。
（之后，用龙和茶壶的例子说明了用顶点着色和像素着色的差异，茶壶不适合用顶点着色，会出现过渡的不自然和一些高光错误）
理论上，可能可以用像素着色器来计算空间中高亮的部分，然后用顶点着色器来计算剩余的部分。但是，在实践中，这种混合应用并不好。将不同部分划分然后再分别计算所带来的额外开销大于它节省的计算量。
顶点着色器大多用于非着色运算，比如转换和形变。
注意，虽然顶点着色器总会产生单位长度的表面法向量，但是插值会改变它们的长度。因此，法向量需要在像素着色器重新单位化。不过顶点着色器产生的法向量长度仍然有影响。比如，如果顶点间的法向量之间差异过大，会导致插值向一边偏斜。因此，通常在插值前和插值后，应用都需要单位化插值的向量，比如，在顶点着色器和像素着色器。
不像法向量，指向特定未知的向量，比如视角向量和准确光照的光线向量，通常不进行插值。
之前我们提到顶点着色器将几何表面变为合适的坐标系统。镜头和光源的位置通常转换到相同的坐标系统传递给像素着色器。那么，什么是“合适”的？可能是全局世界空间，也可能是镜头下的局部坐标系统，或者少数情况下，是当前正在渲染的模型的坐标系。在16.6节会讲关于坐标系的更多细节。

5.3.2 应用举例
Gooch模型的拓展，增加了多个光源
c_shaded = 1/2 * c_cool + Σ(li·n) * c_lighti * (si * c_highlight + (1 - si) * c_warm)
而其中，c_cool = (0, 0, 0.55) + 0.25 * c_surface， c_warm = (0.3, 0.3, 0) + 0.25 * c_surface，c_highlight = (2, 2, 2)，ri = 2 * (n·l)*n - li，si = (100 * (ri·v) - 97).clamp(0, 1)
在大部分的渲染应用中，材料属性的变化值比如c_surface会存储在点数据，或者更常见地，在纹理中（第6章）。
大量光源的渲染技巧会在第20章提到。
在这里，为了简化，我们设c_surface是常量，且所有光源是点光源。
我们将采用WebGL 2中"Phong-shaded Cube"的一个修改版作为例子。
代码环节：
输入输出：
in vec3 vPos;
in vec3 vNormal;
out vec4 outColor;
点光源部分：
struct Light{
	vec4 position;
	vec4 color;
};
uniform LightUBlock{
	Light uLights[MAXLIGHTS];
};

uniform uint uLightCount;

像素着色器部分：
vec3 lit(vec3 l, vec3 n, vec3 v){
	vec3 r_l = reflect(-l, n);
	float s = clamp(100.0 * dot(r_l, v) - 97.0, 0.0, 1.0);
	vec3 highlightColor = vec3(2, 2, 2);
	return mix(uWarmColor, highlightColor, s);
}

void main(){
	vec3 n = normalize(vNormal);
	vec3 v = normalize(uEyePosition.xyz - vPos);
	outColor = vec4(uFUnlit, 1.0);
	
	for(uint i = 0u; i < uLightCount; i ++){
		vec3 l = normalize(uLights[i].position.xyz - vPos);
		float NdL = clamp(dot(n, l), 0.0, 1.0);
		outColor.rgb += NdL * uLights[i].color.rgb * lit(l, n, v);
	}
}
注意，f_unlit()和c_warm是以uniform变量传递的，因为这些量在整个绘制调用中都是常量，这么做可以节省一些GPU周期。

顶点着色器部分，输入输出：
layout(location=0) in vec4 position;
layout(location=1) in vec4 normal;
out vec3 vPos;
out vec3 vNormal;
注意，顶点着色器的输出与像素着色器的输入是匹配的。
顶点着色器部分：
void main() {
	vec4 worldPosition = uModel * position;
	vPos = worldPosition.xyz;
	vNormal = (uModel * normal).xyz;
	gl_Position = viewProj * worldPosition;
}
着色器将表面的点和法向量转换到世界空间中，并将它们传递给像素着色器。最后，表面位置转换到裁剪空间，并传递给gl_Position，一个特别的系统定义的变量，用于光栅化。任何一个顶点着色器都需要输出gl_Position。

像素着色器：
var maxLights = 10;
fSource = fSource.replace(/MAXLIGHTS/g, maxLights.toString());
var fragmentShader = gl.createShader(gl.FRAGMENT_SHADER);
gl.shaderSource(fragmentShader, fSource);
gl.compileShader(fragmentShader);

完整代码：https://github.com/tsherif/webgl2examples

5.3.3 材质系统
材质，一种面向艺术家的表面视觉外观的封装。材质有时也描述一些非视觉方面的内容，比如碰撞参数，不过本书不会做这些内容的讨论。
着色器和材质不是一一对应的关系，同一个材质可能用在多个着色器，同一个着色器也可以用在多种材质上。一些渲染框架，比如虚幻引擎，允许一种更复杂的分层的结构，材质模板可以源自多个层次的其他模板。
参数是可以实时解析的。有时，可能是用户界面的开关，控制一些特质是否包含。有时，可能是在渲染时为了降低开销，而逐渐忽略远处的视觉效果特征。
虽然材质参数可能和着色模型参数有一一对应关系，但并不一定一直如此。
材质系统的最重要的几个任务之一就是将丰富的着色功能拆分为了独立的元素，然后控制它们如何组合。这种组合十分有用，比如： 
用几何处理组合表面着色，比如刚体变换，顶点融合，变形，曲面细分，实例化和裁剪。
用如像素省略和融合这样的操作组合表面着色。这跟移动设备GPU十分相关。
将用于着色模型参数计算的操作和着色模型自身的计算结合。
将独立可选取的材质特征彼此与选取逻辑，着色器剩余部分结合。
将着色模型、着色模型参数的计算与光源评估相结合：对每个光源在着色点上计算c_light和l。
遗憾的是不像CPU代码，GPU着色器并不允许编译后的代码片段链接。每个着色器阶段的程序编译为一个单元。不同着色器阶段的分离提供了有限的模块化，可以让表面着色和几何处理结合。但结合并不完美。由于有这些限制，材质系统唯一能够应用所有类型的组合的办法是在源代码层。这主要包括了字符串运算，比如拼接和替换，通常通过C风格的预编译指令，比如#include, #if和#define。
在设计处理着色变体的系统时，第一个需要解决的问题是选择通过动态分支在运行中演算还是通过条件预处理在编译时演算。老硬件上动态分支常常是不能实现的或者十分慢的，因此运行时选择是不行的。
但是，现代的GPU处理动态分支相当不错，特别是当分支在一次绘制调用中在每个像素上的表现相似时。如今，大部分功能上的变体，比如光的数量，都是在运行中处理的。不过，在着色器中加入大量功能变体会引起不同的开销：寄存器数量的上升和对应的占有率的下降，进而性能表现也会下降。在18.4.5节会有更多细节。因此编译时的变体会更有价值。它避免了将不会执行的复杂逻辑包含在其中。
现代的材质系统同时用到了运行是和编译时着色器变体。即使不再在编译时处理全部问题，整体的复杂度和变体的数量仍然在上升，因此大量的着色器变体仍然需要被编译。虚幻引擎系统有将近10^11个可能的着色器变体。但是只有真正用到的变体会被编译，但是如果要处理大量的变体时，着色器编译系统需要重新设计。
材质系统设计者会用不同的策略达到不同的设计目的。策略通常是在同一个系统中组合的。这些策略如下：
代码重用——在一个共享的文件中应用函数。
精简——一个着色器，通常叫做ubershader或supershader，聚合了一大堆功能，然后用编译时条件预处理和动态分支来将不用的部分移除，以及在互斥的替代者之间切换。
增加——各种功能被定义为用输入和输出连接的节点，且这些可以组合起来。
基于模板的——一个接口定义后，许多应用只要它们能适配那个接口，都可以接入。这通常用于大块功能。
具体的例子可以看WebGL Insights的一些章节。
材质系统还需要确保好的性能。除了对着色变体的特殊编译，还有许多其他优化技巧。

5.4 锯齿和抗锯齿
（这部分内容在GAMES101中详细进行了讲解）

5.4.1 取样和过滤理论

5.4.2 基于屏幕的抗锯齿
对于屏幕上一个像素的颜色p：
p(x, y) = Σwi * c(i, x, y)，其中Σwi = 1，c可以视作两个函数的组合，一个是从f(i, n)找到需要取样的屏幕上的点(x, y)，然后在(x, y)取样，溯源到准确的颜色。
在实时渲染中，wi一般取1 / n。
一种实时抗锯齿的方法是NVIDIA的梅花取样法。具体取样是4个角4个点，中间1个点，中间的点权重为1/2，其余点权重是1/8。

形态学方法
抗锯齿也可以在渲染完成后再做，找到边缘并重建边缘。基于这种想法的抗锯齿方法有SRAA(subpixel reconstruction antialiasing)，GBAA(geometry buffer antialiasing)和DEAA(distance-to-edge antialiasing)。最常用的方法是只需要颜色缓存，比如directionally localized antialiasing(DLAA)。
两种最流行的算法是FXAA(fast approximate antialiasing)和SMAA(subpixel morphological antialiasing)，部分是因为两者都为很多机器提供了扎实（且免费）的源代码应用。两者都是只用颜色的输入，SMAA还可以利用MSAA的样本。两者都有自己的很多设置，在速度和质量之间权衡。开销通常是每帧1-2毫秒，主要是因为这是电子游戏期望的值。最后，两者都利用了时域抗锯齿。最后总结，我们建议读者取回顾Reshetov和Jimenez的形态学方法，以及它们在电子游戏中的应用。

5.5 透明度，Alpha和合成
光线通过半透明物体的方法有很多种。对于渲染算法，这些大致可以分为基于光线的和基于视觉的效果。基于光线的效果指的是那些引起光线变弱或变向进而让场景中其他物体被照亮，从而渲染起来不同。基于视觉效果指的是那些半透明物体自身渲染时的情况。这一节，我们将讲基于视觉的透明的最简单形式，将半透明物体等效为一个对其后面物体生效的颜色减弱器。更复杂的一些情况会在后面的章节讨论。
一种给出透明效果的办法叫做纱门透明度。这个想法是用一种像素对齐的棋盘填充的方式渲染透明三角形。就是每隔一个三角形渲染一个三角形，然后让三角形后面的物体部分可见。通常屏幕上的像素足够密集，足以让棋盘模式自身不可见。缺点是，只能在屏幕的一个区域渲染一个透明物体，多了就会失真。好处是，这个方法简单。
在子像素级别，用到了一个特性叫做alpha to coverage(覆盖率的alpha)。
大多数的透明度算法进透明物体的颜色和其后面物体的颜色混合。为了这一点，alpha混合的概念产生了。Alpha是一个描述不透明程度和给定像素的物体碎片覆盖率的值。Alpha为1表示物体不透明且完全覆盖了像素区域。0意味着像素完全没有被遮挡。
Alpha值可以表示不透明度，遮挡率或者两者皆有。

5.5.1 混合顺序
将碎片值和原像素颜色混合通常用over运算操作：
c_o = α_s * c_s + (1 − α_s) * c_d
c_s是透明物体的颜色（叫做source），α_s是物体的alpha值，c_d是混合前的像素颜色（叫做destination），c_o是将透明物体放到已有场景之上的结果。
另一种有时用到的运算是additive blending，叠加混合，就是简单地求和，即c_o = α_s * c_s + c_d，这个公式对发光的效果表现较好。
在任何给定的像素上使用over透明表面的运算时，都需要按从后到前的顺序渲染。
如果叠加顺序从前向后，我们可以用under操作：
c_o = α_d * c_d + (1 − α_d) * α_s * c_s
a_o = α_s * (1 − α_d) + α_d = α_s − α_s * α_d + α_d

5.5.2 顺序无关的透明
OIT(order-independent transparency)算法，也叫做depth peeling(深度分离)，其思想是用两个z-buffer和多轮扫描。首先，一轮渲染进行，得到表面的z-buffer。然后第二轮，所有的透明物体被渲染，如果被渲染的物体的z值与z-buffer中的一致，说明这个物体是距离最近的物体，将它的RGBα存入一个独立的color buffer。我们同时将这层去掉，并将去掉这层后最近的透明物体的深度保留，即第二近的透明物体。如此迭代，每次我们将最近的透明层去掉并加入新的最近的透明层，直到遇到不透明层。
混合透明物体并不缺算法，而缺能高效将算法用到GPU的映射。

5.5.3 预乘的Alpha和合成
over运算也可以用于合成照片或者合成物体渲染。这个过程叫做合成(compositing)

5.6 显示编码
显示编码和纹理用的是非线性编码，对于这一点我们必须要纳入考量。一个简单而粗糙的答案是将输出的颜色取值到[0, 1]并对其用1/2.2次方乘方，这也叫做gamma校正。

第6章 纹理
表面的纹理就是它的外观和感觉，想一下油画的纹理。在计算机图形学中，纹理是用图片、函数或其他数据，对每个位置修改器表面外观的过程。比如砖墙。

6.1 纹理管线
纹理是一种能高效模型化表面材质变化并高效完成的技术。
纹理管线如下：
input: 对象空间的位置（object space location），process: 投影函数（projector function）
input：参数空间的坐标（parameter space coordinates），process: 对应函数（corresponder function(s)）
input：纹理空间的位置（texture space location），process：获取值（obtain value）
input：纹理值（texture value），process：值转换函数（value transform function）
output：转换后的纹理值（transformed texture value）

6.1.1 投影函数（projector function）
纹理过程的第一步是获取表面的位置然后将其投影到纹理坐标空间，通常是二维空间(u, v)。常用的函数包括：球形、柱形和平面投影。
其他的一些投影函数实际上完全不是投影，而是特定的表面生成和表面细分的一部分。
非交互渲染器经常调用这些投影函数，并将其视作渲染自身的一部分。
在实时渲染中，投影函数通常在模型化阶段使用，而投影的结果存储在顶点中。不过，有时可能会在顶点或像素着色器中应用投影函数。
展开网格的过程是一个更大的研究领域————网格参数化的一个方面。做一个好的网格展开可以减轻艺术家的工作。
纹理坐标不一定是二维的，有时可以是三维的(u, v, w)，w是投影方向的深度。一些其他的系统用四维坐标，通常是(s, t, r, q)，q的用法就像齐次坐标中的第四维。投影纹理的大小会随距离增长。应用举例，gobo。
另一种纹理坐标空间的重要类型是方向性的。想象这个空间是一个单位球，球上的每个点的法向量可以用于获取那个位置的纹理。举例，球形映射(cube map)。
此外，一维的纹理图片和函数也有其作用。

6.1.2 对应函数（corresponder function）
对应函数将纹理坐标转换为纹理空间的位置。它给将纹理应用到表面提供了灵活性。一个例子，对应函数可以用于选择一部分已经存在的纹理将其展示的API。
另一种对应函数类型是矩阵转换，可以用于顶点或像素着色器。
还有一类对应函数控制了图片应用的方式。常见的这类对应函数有：
wrap(DirectX)，repeat(OpenGL)或tile：图片在表面重复自身，算法层面就是纹理坐标的整数部分被去掉。
mirror：图片在表面重复但是每次重复都做一次镜像。
clamp(DirectX)或clamp to edge(OpenGL)：[0, 1]之外的值会取到这个区间中。
border(DirectX)或clamp to border(OpenGL)：[0, 1]之外的值会渲染为一个单独定义的边界颜色。
解决重复性的方法：
1)将纹理值和另一个非平铺的纹理结合。
2)用一个特殊的对应函数可以随机地结合纹理模式。例如Wang tiles。
最后还有一种对应函数是隐函数，根据图片的规模进行映射。这种对应函数的好处是可以将不同分辨率的图片纹理直接替换而不必改变模型中顶点中存储的值。

6.1.3 纹理值
最直接的值是RGB三元组，用于替换或修改表面颜色。另一种返回的数据是RGBα，在5.5章节已经提过，α值通常是颜色的不透明度。
从纹理返回的值可以在用之前进行转换。这些转换可以在着色程序中进行。一种常见的例子是将(0, 1)区间的值映射到(-1, 1)

6.2 图片纹理
图片纹理，二维的图片被高效地贴到一个或多个三角形表面。
在本章，我们会重点讲快速取样和过滤纹理化图片的方法。
像素着色器通过将纹理坐标值传入一个调用，比如texture2D，来获取纹理。在DirectX中纹理的左上角是(0, 0)，右下是(1, 1)。在OpenGL中左下角是(0, 0)。像素中心的坐标是什么，有两种不同的系统：取整或截断。
另一个需要解释的术语是dependent texture read（依赖纹理读入），有两种定义。第一种专用于移动设备，表示在像素着色器计算纹理坐标而不是直接用没有修改的纹理坐标。另一种是早期定义，用于桌面GPU，当纹理坐标与上一个纹理值相关的时候，称为依赖纹理读入。

6.2.1 放大
放大纹理的常用方法：nearest neighbor（用到的过滤器是box filter，见5.4.1）和双线性插值。也可以用三次卷积（cubic convolution）。虽然初始的硬件并大多并不支持三次卷积，但是它可以在着色器程序中实现。

6.2.2 缩小
缩小的时候，多个纹素(texel)可能覆盖同一个像素单元，这样就难以得到每个像素正确的颜色。
为了解决这个问题，GPU上用了很多不同的方法。一个是nearest neighbor，比如选择像素单元中心的纹素。但这会引起严重的走样问题。
另一种过滤器是双线性差值，跟放大的过滤器原理相同。这个效果只比nearest neighbor好一点点。
更好的解决办法是用5.4.1中抗锯齿的办法。提高像素的采样频率或者降低纹理的频率。不过要解决好这个问题，有很多算法被开发了出来。
这些纹理抗锯齿算法的一个基础想法是：预处理纹理，创建一个数据结构可以快速求出一个像素内一组纹素的近似效果。
Mipmapping
最流行的像素抗锯齿方法是mipmapping。
（这个部分的内容在GAMES101中详细进行了讲解）
Summed-Area Table
（这部分内容在GAMES202中也有讲解）
无约束的各向异性过滤（Unconstrained Anisotropic Filtering）

6.2.3 体积纹理
大多数GPU支持体积纹理的mipmapping。不过其中用到的就是层次内的三线性差值，和层次间的四线性插值。

6.2.4 球形映射
对于球形纹理或球形映射。球形纹理通过一个三元纹理坐标，用于指定从球心向外的方向来确定。

6.2.5 纹理表示
有很多在处理许多纹理时提升性能的方法。纹理压缩在6.2.6中会提到。纹理流和转码会在19.10.1和19.10.2中提到。
为了减少状态的改变，我们可以将多个图片放入一个纹理图集（texture atlas）。
不过纹理图集的一个麻烦在于重复或镜像模式难以实现。另一个问题是给一个纹理图集生成mipmap时可能遇到问题。不过这个可以通过在进入纹理图集前就给每个纹理生成好mipmap来解决。
解决上述问题的一个办法是用一个叫做texture arrays的API。

6.2.6 纹理压缩
压缩的纹理可以减少需要用的内存进而提升性能。
为压缩的方法：S3 Texture Compression(S3TC)，这也是DirectX的标准，叫做DXTC，在DirectX 10中叫做BC(Block Compression)。事实上它也是OpenGL的标准，因为几乎所有GPU支持它。
对于OpenGL ES，另一种压缩算法，叫做Ericsson texture compression（ETC）。这个方法有与S3TC相同的特征，即快速解码，随机获取，没有间接查找，且速率可调。

6.3 程序纹理
用一个函数生成纹理。常用于离线渲染，因为纹理贴图在GPU上的表现太好。
不过，程序纹理对体积纹理而言是个不错的应用，因为体积纹理空间开销比较高。如果能用一些方法生成体积纹理那就太好了。最常用的方法之一是用noise function生成值。
（Perlin，这里提到了这位大神！）

6.4 纹理动画
纹理坐标不需要固定不变。应用的设计者可以让纹理坐标每帧都发生具体的变化。

6.5 材质映射
纹理的一个常见用法是修改材质的一些参数进而影响着色方程的结果。

6.6 Alpha映射
alpha值可以用于许多效果，比如alpha混合或alpha测试，比如高效渲染叶子、爆炸或远处物体。这个章节讲一下Alpha贴图，以及其局限性和解决办法。
一个纹理相关的效果是贴花。比如，理想将一朵花贴到一个茶壶上，你可以通过调整alpha值，来将不想贴的部分调成透明。
一个类似的应用是剪纸。跟贴花原理相同。

6.7 法线贴图（bump mapping）
法线贴图是小尺度细节表示技术的集合。所有这些方法都用于修改每个像素的常规着色。
物体的细节可以分为三个尺度：macro-features，覆盖多个像素；meso-features，几个像素；micro-features，小于一个像素。
Macro几何可以用点和三角形或者其他几何原型表示。
Micro几何封装在着色模型中，常用于像素着色器，将纹理映射作为参数。
Meso几何描述的是介于前两者之间的东西。
Blinn发现，如果我们在渲染时将物体表面法向量进行微扰，物体就会呈现出一些很小尺度上的细节。
核心想法是，用贴图来修改物体表面的法向量。

6.7.1 Blinn的方法

6.7.2 法向量映射

（这部分内容在GAMES101中有详细讲解，且有对应的作业）

6.8 视差贴图
（个人理解，法线贴图真实增强版，真实地改变物体表面的凹凸，产生遮挡之类的效果）
（GAMES101中有详细讲解）

6.9 纹理光照
纹理还可以用于添加光源的视觉丰富度，允许复杂的强度分布或光源函数。对于锥体或者截头锥体光源，可以用投影贴图来修饰光强。
对于非截头锥体但各向都照亮的光源，球形贴图可以用于修饰光强。一维距离跌落函数，结合二维的角度削弱贴图，可以产生复杂的体积光模式。






































